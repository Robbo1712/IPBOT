import os
import time
import configparser
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import ccxt
import telegram
import logging
from logging.handlers import RotatingFileHandler
from sklearn.ensemble import RandomForestClassifier # For ML Scoring
from sklearn.cluster import KMeans # For Market Regime
from sklearn.preprocessing import StandardScaler
import joblib # For saving/loading ML models
import tradingpatterns # For chart patterns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import requests # For potential news fetching
from bs4 import BeautifulSoup # For potential news fetching (example)


# --- Initial Setup for NLTK (run this once interactively if needed) ---
# try:
#     nltk.data.find('sentiment/vader_lexicon.zip')
# except nltk.downloader.DownloadError:
#     nltk.download('vader_lexicon')
# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords')
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt')
# ---------------------------------------------------------------------

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler("trading_bot_enhanced.log", maxBytes=10485760, backupCount=5, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("trading_bot_enhanced")

class KuCoinSwingBotEnhanced:
    def __init__(self, config_file="config.txt"):
        """Initialize the trading bot with configuration."""
        self.config = self._load_config(config_file)
        self.exchange = self._setup_exchange()
        self.telegram_bot = self._setup_telegram()

        self.symbols = self._get_top_usdt_symbols(top_n=self.config.getint('TRADING', 'TOP_N_SYMBOLS', fallback=20))
        self.timeframes = self.config.get('TRADING', 'TIMEFRAMES', fallback='1h,4h,1d').split(',')
        self.default_ohlcv_timeframe = self.config.get('TRADING', 'DEFAULT_OHLCV_TIMEFRAME', fallback='1h')
        
        # Indicator parameters
        self.short_window = self.config.getint('INDICATORS', 'EMA_SHORT_WINDOW', fallback=9)
        self.long_window = self.config.getint('INDICATORS', 'EMA_LONG_WINDOW', fallback=21)
        self.rsi_period = self.config.getint('INDICATORS', 'RSI_PERIOD', fallback=14)
        self.rsi_overbought = self.config.getint('INDICATORS', 'RSI_OVERBOUGHT', fallback=70)
        self.rsi_oversold = self.config.getint('INDICATORS', 'RSI_OVERSOLD', fallback=30)
        self.bollinger_period = self.config.getint('INDICATORS', 'BOLLINGER_PERIOD', fallback=20)
        self.bollinger_std = self.config.getfloat('INDICATORS', 'BOLLINGER_STD', fallback=2.0)
        self.macd_fast = self.config.getint('INDICATORS', 'MACD_FAST', fallback=12)
        self.macd_slow = self.config.getint('INDICATORS', 'MACD_SLOW', fallback=26)
        self.macd_signal_smooth = self.config.getint('INDICATORS', 'MACD_SIGNAL_SMOOTH', fallback=9)
        self.stoch_k_period = self.config.getint('INDICATORS', 'STOCH_K_PERIOD', fallback=14)
        self.stoch_d_period = self.config.getint('INDICATORS', 'STOCH_D_PERIOD', fallback=3)
        self.adx_period = self.config.getint('INDICATORS', 'ADX_PERIOD', fallback=14)
        self.atr_period_risk = self.config.getint('INDICATORS', 'ATR_PERIOD_RISK', fallback=14) # ATR for risk
        self.volume_ma_period = self.config.getint('INDICATORS', 'VOLUME_MA_PERIOD', fallback=20)

        # Trading strategy parameters
        self.min_pattern_reliability_score = self.config.getfloat('STRATEGY', 'MIN_PATTERN_RELIABILITY_SCORE', fallback=0.6)
        self.min_sentiment_score_long = self.config.getfloat('STRATEGY', 'MIN_SENTIMENT_SCORE_LONG', fallback=0.05) # VADER compound
        self.max_sentiment_score_short = self.config.getfloat('STRATEGY', 'MAX_SENTIMENT_SCORE_SHORT', fallback=-0.05) # VADER compound
        self.regime_specific_logic = self.config.getboolean('STRATEGY', 'REGIME_SPECIFIC_LOGIC', fallback=True)


        # Risk management parameters
        self.default_stop_loss_atr_multiplier = self.config.getfloat('RISK', 'DEFAULT_SL_ATR_MULTIPLIER', fallback=1.5)
        self.default_tp_atr_multiplier = self.config.getfloat('RISK', 'DEFAULT_TP_ATR_MULTIPLIER', fallback=2.0) # For TP1
        self.tp2_atr_multiplier = self.config.getfloat('RISK', 'TP2_ATR_MULTIPLIER', fallback=3.0)
        self.tp3_atr_multiplier = self.config.getfloat('RISK', 'TP3_ATR_MULTIPLIER', fallback=4.5)
        self.trailing_stop_atr_multiplier = self.config.getfloat('RISK', 'TRAILING_SL_ATR_MULTIPLIER', fallback=1.0)
        self.enable_trailing_stop = self.config.getboolean('RISK', 'ENABLE_TRAILING_STOP', fallback=True)
        self.risk_per_trade_pct = self.config.getfloat('RISK', 'RISK_PER_TRADE_PCT', fallback=0.01) # 1% of balance
        self.max_concurrent_trades = self.config.getint('RISK', 'MAX_CONCURRENT_TRADES', fallback=5)


        # Data structures
        self.trade_history = {symbol: [] for symbol in self.symbols}
        self.signals_generated_count = {symbol: 0 for symbol in self.symbols} # Renamed for clarity
        self.successful_signals_count = {symbol: 0 for symbol in self.symbols} # Renamed for clarity
        self.active_trades = {symbol: [] for symbol in self.symbols} # Will store more detailed trade objects
        self.last_signal_time = {symbol: None for symbol in self.symbols}
        self.last_signal_direction = {symbol: 0 for symbol in self.symbols}
        
        self.price_history_files = {symbol: f"price_history_{symbol.replace('/', '_')}.json" for symbol in self.symbols}
        self.price_history = {symbol: {"swings": [], "trade_results": [], "historical_features": []} for symbol in self.symbols} # Added historical_features
        
        for symbol in self.symbols:
            self.load_price_history(symbol)

        # Chart Pattern Recognition Setup
        self.pattern_client = StockMarket(country="US") # tradingpatternlibrary

        # ML Model Placeholders (to be loaded or trained)
        self.ml_reliability_model = None
        self.ml_model_scaler = None # For scaling features
        self.market_regime_model = None # For K-Means
        self.market_regime_scaler = None # For K-Means features
        self._load_ml_models() # Attempt to load pre-trained models

        # Sentiment Analysis Setup
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.stop_words = set(stopwords.words('english'))

        logger.info(f"Bot initialized for: {', '.join(self.symbols)} on {self.timeframes}. Primary TF: {self.default_ohlcv_timeframe}")
        self.send_telegram_info(f"ðŸš€ Enhanced Trading Bot Started\nSymbols: {len(self.symbols)}\nTimeframes: {', '.join(self.timeframes)}")

    def _load_config(self, config_file):
        config = configparser.ConfigParser(inline_comment_prefixes='#', allow_no_value=True)
        if not os.path.exists(config_file):
            logger.error(f"Config file not found: {config_file}. Creating default.")
            self._create_default_config(config_file) # Create a default config if not found
        config.read(config_file)
        
        # Ensure sections exist, add them if not
        required_sections = ['KUCOIN', 'TELEGRAM', 'TRADING', 'INDICATORS', 'STRATEGY', 'RISK', 'ML_MODELS', 'SENTIMENT']
        for section in required_sections:
            if not config.has_section(section):
                config.add_section(section)
                logger.info(f"Added missing section [{section}] to config.")

        # Set some default values if missing to avoid errors later
        # Example: config.set('TRADING', 'TOP_N_SYMBOLS', config.get('TRADING', 'TOP_N_SYMBOLS', fallback='20'))
        # This should be done for all critical parameters or handled by .get(fallback=...)

        return config

    def _create_default_config(self, config_file):
        config = configparser.ConfigParser(inline_comment_prefixes='#', allow_no_value=True)
        config['KUCOIN'] = {
            'API_KEY': 'YOUR_KUCOIN_API_KEY',
            'API_SECRET': 'YOUR_KUCOIN_API_SECRET',
            'API_PASSPHRASE': 'YOUR_KUCOIN_API_PASSPHRASE'
        }
        config['TELEGRAM'] = {
            'BOT_TOKEN': 'YOUR_TELEGRAM_BOT_TOKEN',
            'CHAT_ID': 'YOUR_TELEGRAM_CHAT_ID'
        }
        config['TRADING'] = {
            'TOP_N_SYMBOLS': '20',
            'TIMEFRAMES': '1h,4h,1d',
            'DEFAULT_OHLCV_TIMEFRAME': '1h',
            'MIN_CANDLES_FOR_SIGNAL': '50' # Min candles for primary TF for signal generation
        }
        config['INDICATORS'] = {
            'EMA_SHORT_WINDOW': '9', 'EMA_LONG_WINDOW': '21', 'RSI_PERIOD': '14',
            'RSI_OVERBOUGHT': '70', 'RSI_OVERSOLD': '30', 'BOLLINGER_PERIOD': '20',
            'BOLLINGER_STD': '2.0', 'MACD_FAST': '12', 'MACD_SLOW': '26',
            'MACD_SIGNAL_SMOOTH': '9', 'STOCH_K_PERIOD': '14', 'STOCH_D_PERIOD': '3',
            'ADX_PERIOD': '14', 'ATR_PERIOD_RISK': '14', 'VOLUME_MA_PERIOD': '20'
        }
        config['STRATEGY'] = {
            'MIN_PATTERN_RELIABILITY_SCORE': '0.6', # For ML scoring
            'MIN_SENTIMENT_SCORE_LONG': '0.05', # For VADER sentiment
            'MAX_SENTIMENT_SCORE_SHORT': '-0.05', # For VADER sentiment
            'REGIME_SPECIFIC_LOGIC': 'True',
            'ENABLE_CHART_PATTERNS': 'True',
            'ENABLE_ML_SCORING': 'False', # Default to False until model is trained
            'ENABLE_MARKET_REGIME': 'False', # Default to False until model is trained
            'ENABLE_SENTIMENT_ANALYSIS': 'False' # Default to False due to complexity
        }
        config['RISK'] = {
            'DEFAULT_SL_ATR_MULTIPLIER': '1.5', 'DEFAULT_TP_ATR_MULTIPLIER': '2.0',
            'TP2_ATR_MULTIPLIER': '3.0', 'TP3_ATR_MULTIPLIER': '4.5',
            'TRAILING_SL_ATR_MULTIPLIER': '1.0', 'ENABLE_TRAILING_STOP': 'True',
            'RISK_PER_TRADE_PCT': '0.01', # 1% of account balance
            'MAX_CONCURRENT_TRADES': '5',
            'PARTIAL_TP_PERCENTAGES': '0.33,0.33,0.34' # e.g. 33% at TP1, 33% at TP2, 34% at TP3
        }
        config['ML_MODELS'] = {
            'RELIABILITY_MODEL_PATH': 'ml_reliability_model.joblib',
            'RELIABILITY_SCALER_PATH': 'ml_reliability_scaler.joblib',
            'REGIME_MODEL_PATH': 'market_regime_model.joblib',
            'REGIME_SCALER_PATH': 'market_regime_scaler.joblib'
        }
        config['SENTIMENT'] = {
            'NEWS_API_KEY': 'YOUR_NEWS_API_KEY', # Placeholder for newsapi.org or similar
            'SENTIMENT_DATA_SOURCE': 'VADER_GENERIC' # Options: VADER_GENERIC, NEWS_API, TWITTER (latter two need dev)
        }
        with open(config_file, 'w') as f:
            config.write(f)
        logger.info(f"Created default configuration file: {config_file}")


    def _setup_exchange(self):
        try:
            exchange = ccxt.kucoin({
                'apiKey': self.config.get('KUCOIN', 'API_KEY'),
                'secret': self.config.get('KUCOIN', 'API_SECRET'),
                'password': self.config.get('KUCOIN', 'API_PASSPHRASE'),
                'enableRateLimit': True,
                'options': {'adjustForTimeDifference': True} 
            })
            logger.info("KuCoin exchange connection established")
            return exchange
        except Exception as e:
            logger.error(f"Failed to set up KuCoin exchange: {str(e)}")
            raise

    def _setup_telegram(self):
        try:
            bot = telegram.Bot(token=self.config.get('TELEGRAM', 'BOT_TOKEN'))
            logger.info("Telegram bot initialized")
            return bot
        except Exception as e:
            logger.error(f"Failed to set up Telegram bot: {str(e)}")
            # Allow bot to run without Telegram for testing if needed, but log error
            return None
            
    def send_telegram_info(self, message):
        if not self.telegram_bot:
            logger.warning(f"Telegram bot not initialized. Message not sent: {message[:50]}...")
            return
        try:
            self.telegram_bot.send_message(
                chat_id=self.config.get('TELEGRAM', 'CHAT_ID'),
                text=message,
                parse_mode=telegram.ParseMode.MARKDOWN
            )
            logger.info(f"Telegram info message sent: {message[:50]}...")
        except Exception as e:
            logger.error(f"Failed to send Telegram info message: {str(e)}")

    def send_signal_message(self, message):
        if not self.telegram_bot:
            logger.warning(f"Telegram bot not initialized. Signal message not sent: {message[:50]}...")
            return None
        try:
            sent_message = self.telegram_bot.send_message(
                chat_id=self.config.get('TELEGRAM', 'CHAT_ID'),
                text=message,
                parse_mode=telegram.ParseMode.MARKDOWN
            )
            logger.info(f"Signal message sent: {message[:50]}...")
            return sent_message
        except Exception as e:
            logger.error(f"Failed to send signal message: {str(e)}")
            return None
    
    def get_telegram_message_link(self, chat_id, message_id):
        if not chat_id or not message_id: return ""
        try:
            # Ensure chat_id is string and handle potential -100 prefix for supergroups
            chat_id_str = str(chat_id)
            if chat_id_str.startswith('-100'):
                # Format for public supergroups: t.me/c/channel_id/message_id
                # channel_id is chat_id without the -100 prefix
                return f"https://t.me/c/{chat_id_str[4:]}/{message_id}"
            elif chat_id_str.startswith('-'):
                 # Private groups/supergroups: t.me/c/group_id/message_id (group_id is chat_id without '-')
                 # This might not always create a clickable link directly, depends on Telegram's handling
                return f"https://t.me/c/{chat_id_str[1:]}/{message_id}" # A common format
            else:
                # For direct chats or other types, this might be more complex
                # Fallback for other cases, may not be universally linkable
                return f"https://t.me/{chat_id}/{message_id}" # This usually doesn't work for groups
        except Exception as e:
            logger.error(f"Error generating Telegram link for chat {chat_id} msg {message_id}: {e}")
            return ""


    def _get_top_usdt_symbols(self, top_n=20):
        try:
            tickers = self.exchange.fetch_tickers()
            usdt_tickers = []
            for symbol, tick in tickers.items():
                if symbol.endswith('/USDT') and tick.get('quoteVolume') is not None:
                     # Exclude leveraged tokens or other non-spot markets if possible
                    if 'UP/' not in symbol and 'DOWN/' not in symbol and '3S/' not in symbol and '3L/' not in symbol and '5S/' not in symbol and '5L/' not in symbol:
                        usdt_tickers.append((symbol, tick['quoteVolume']))
            
            usdt_tickers.sort(key=lambda x: x[1], reverse=True)
            top_symbols = [symbol for symbol, vol in usdt_tickers[:top_n]]
            if not top_symbols:
                logger.warning("No top USDT symbols found, using fallback.")
                return self._get_all_usdt_symbols(limit=top_n)
            logger.info(f"Fetched top {len(top_symbols)} USDT symbols by volume.")
            return top_symbols
        except Exception as e:
            logger.error(f"Failed to fetch top USDT symbols: {str(e)}. Using fallback.")
            return self._get_all_usdt_symbols(limit=top_n)

    def _get_all_usdt_symbols(self, limit=20):
        try:
            markets = self.exchange.load_markets()
            usdt_symbols = [
                symbol for symbol, market_data in markets.items()
                if symbol.endswith('/USDT') and market_data.get('active', False) and market_data.get('spot', False)
                and 'UP/' not in symbol and 'DOWN/' not in symbol and '3S/' not in symbol and '3L/' not in symbol
            ]
            logger.info(f"Fetched {len(usdt_symbols)} active USDT spot symbols. Limiting to {limit}.")
            return usdt_symbols[:limit] if usdt_symbols else ["BTC/USDT", "ETH/USDT"]
        except Exception as e:
            logger.error(f"Failed to fetch all USDT symbols: {str(e)}")
            return ["BTC/USDT", "ETH/USDT"]

    def load_price_history(self, symbol):
        try:
            filename = self.price_history_files[symbol]
            if os.path.exists(filename):
                with open(filename, 'r') as f:
                    loaded_history = json.load(f)
                    # Ensure all keys exist
                    self.price_history[symbol] = {
                        "swings": loaded_history.get("swings", []),
                        "trade_results": loaded_history.get("trade_results", []),
                        "historical_features": loaded_history.get("historical_features", []) 
                    }
                logger.info(f"Loaded price history for {symbol}: {len(self.price_history[symbol]['swings'])} swings, {len(self.price_history[symbol]['trade_results'])} trades, {len(self.price_history[symbol]['historical_features'])} feature sets.")
            else:
                self.price_history[symbol] = {"swings": [], "trade_results": [], "historical_features": []}
                logger.info(f"Initialized new price history for {symbol}")
        except Exception as e:
            logger.error(f"Error loading price history for {symbol}: {str(e)}")
            self.price_history[symbol] = {"swings": [], "trade_results": [], "historical_features": []}


    def save_price_history(self, symbol):
        try:
            filename = self.price_history_files[symbol]
            with open(filename, 'w') as f:
                json.dump(self.price_history[symbol], f, indent=4)
            logger.debug(f"Price history saved for {symbol}") # Changed to debug to reduce log spam
        except Exception as e:
            logger.error(f"Error saving price history for {symbol}: {str(e)}")

    def _ohlcv_to_dataframe(self, ohlcv, symbol, timeframe):
        if not ohlcv:
            logger.warning(f"No OHLCV data returned for {symbol} on {timeframe}.")
            return pd.DataFrame()
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        df.dropna(subset=['open', 'high', 'low', 'close', 'volume'], inplace=True) # Drop rows where essential OHLCV is NaN
        return df

    def fetch_ohlcv_data(self, symbol, timeframe='1h', limit=200): # Increased default limit
        try:
            if not isinstance(symbol, str):
                raise ValueError(f"Invalid symbol type for fetch_ohlcv_data: {type(symbol)}. Expected str.")
            
            # KuCoin specific: check if market exists and is active
            if symbol not in self.exchange.markets:
                logger.warning(f"Market {symbol} not found in exchange markets. Attempting to load markets.")
                self.exchange.load_markets(True) # Force reload
                if symbol not in self.exchange.markets:
                    logger.error(f"Market {symbol} still not found after reloading markets.")
                    return pd.DataFrame()
            
            if not self.exchange.markets[symbol].get('active', True): # Assume active if not specified
                logger.warning(f"Market {symbol} is not active.")
                return pd.DataFrame()

            ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
            df = self._ohlcv_to_dataframe(ohlcv, symbol, timeframe)
            
            logger.info(f"Fetched {len(df)} candlesticks for {symbol} on {timeframe}")
            return df
        except ccxt.BadSymbol:
            logger.error(f"BadSymbol: Could not fetch OHLCV for '{symbol}' on {timeframe}. Invalid or delisted.")
            return pd.DataFrame()
        except ccxt.NetworkError as e:
            logger.error(f"NetworkError fetching OHLCV for {symbol} on {timeframe}: {e}")
            return pd.DataFrame() # Allow retry later
        except ccxt.ExchangeError as e: # More specific CCXT errors
            logger.error(f"ExchangeError fetching OHLCV for {symbol} on {timeframe}: {e}")
            return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error fetching OHLCV for {symbol} on {timeframe}: {str(e)}")
            return pd.DataFrame()

    def calculate_indicators(self, df):
        if df.empty or len(df) < max(self.long_window, self.rsi_period, self.bollinger_period, self.macd_slow, self.stoch_k_period, self.adx_period, self.atr_period_risk, self.volume_ma_period):
            logger.warning(f"DataFrame is too short ({len(df)} rows) for full indicator calculation.")
            return pd.DataFrame() # Return empty if not enough data for all indicators
            
        data = df.copy()
        data['ema_short'] = data['close'].ewm(span=self.short_window, adjust=False).mean()
        data['ema_long'] = data['close'].ewm(span=self.long_window, adjust=False).mean()
        
        delta = data['close'].diff()
        gain = (delta.where(delta > 0, 0.0)).ewm(alpha=1/self.rsi_period, adjust=False).mean() # Using Wilder's smoothing
        loss = (-delta.where(delta < 0, 0.0)).ewm(alpha=1/self.rsi_period, adjust=False).mean() # Using Wilder's smoothing
        
        rs = gain / loss.where(loss != 0, np.nan) # Avoid division by zero
        data['rsi'] = 100 - (100 / (1 + rs))
        data['rsi'].fillna(50, inplace=True) # Fill NaNs (e.g., if loss is 0 for a period) with neutral 50

        data['sma_bb'] = data['close'].rolling(window=self.bollinger_period).mean() # Renamed for clarity
        data['std_bb'] = data['close'].rolling(window=self.bollinger_period).std()
        data['upper_band'] = data['sma_bb'] + (data['std_bb'] * self.bollinger_std)
        data['lower_band'] = data['sma_bb'] - (data['std_bb'] * self.bollinger_std)
        
        data['ema_macd_fast'] = data['close'].ewm(span=self.macd_fast, adjust=False).mean()
        data['ema_macd_slow'] = data['close'].ewm(span=self.macd_slow, adjust=False).mean()
        data['macd'] = data['ema_macd_fast'] - data['ema_macd_slow']
        data['macd_signal'] = data['macd'].ewm(span=self.macd_signal_smooth, adjust=False).mean()
        data['macd_hist'] = data['macd'] - data['macd_signal']
        
        data['volume_ma'] = data['volume'].rolling(window=self.volume_ma_period).mean()
        # Avoid division by zero for volume_change, handle cases where volume_ma is 0
        data['volume_change_pct'] = ((data['volume'] - data['volume_ma']) / data['volume_ma'].where(data['volume_ma'] != 0, np.nan) * 100).fillna(0)


        low_min_stoch = data['low'].rolling(window=self.stoch_k_period).min()
        high_max_stoch = data['high'].rolling(window=self.stoch_k_period).max()
        # Avoid division by zero if high_max_stoch == low_min_stoch
        stoch_range = high_max_stoch - low_min_stoch
        data['stoch_k'] = (100 * (data['close'] - low_min_stoch) / stoch_range.where(stoch_range != 0, np.nan)).fillna(50)
        data['stoch_d'] = data['stoch_k'].rolling(window=self.stoch_d_period).mean()

        # ADX Calculation (using Wilder's smoothing for consistency)
        df_adx = pd.DataFrame(index=data.index)
        df_adx['high'] = data['high']
        df_adx['low'] = data['low']
        df_adx['close'] = data['close']

        df_adx['tr1'] = df_adx['high'] - df_adx['low']
        df_adx['tr2'] = abs(df_adx['high'] - df_adx['close'].shift(1))
        df_adx['tr3'] = abs(df_adx['low'] - df_adx['close'].shift(1))
        df_adx['tr'] = df_adx[['tr1', 'tr2', 'tr3']].max(axis=1)
        
        df_adx['atr_adx'] = df_adx['tr'].ewm(alpha=1/self.adx_period, adjust=False).mean() # ATR for ADX

        df_adx['high_diff'] = df_adx['high'].diff()
        df_adx['low_diff'] = df_adx['low'].diff()
        df_adx['plus_dm_val'] = 0.0
        df_adx.loc[(df_adx['high_diff'] > -df_adx['low_diff']) & (df_adx['high_diff'] > 0), 'plus_dm_val'] = df_adx['high_diff']
        df_adx['minus_dm_val'] = 0.0
        df_adx.loc[(-df_adx['low_diff'] > df_adx['high_diff']) & (-df_adx['low_diff'] > 0), 'minus_dm_val'] = -df_adx['low_diff']

        df_adx['plus_di'] = 100 * (df_adx['plus_dm_val'].ewm(alpha=1/self.adx_period, adjust=False).mean() / df_adx['atr_adx'].where(df_adx['atr_adx'] != 0, np.nan))
        df_adx['minus_di'] = 100 * (df_adx['minus_dm_val'].ewm(alpha=1/self.adx_period, adjust=False).mean() / df_adx['atr_adx'].where(df_adx['atr_adx'] != 0, np.nan))
        
        dx_denominator_adx = (df_adx['plus_di'] + df_adx['minus_di'])
        df_adx['dx'] = 100 * abs(df_adx['plus_di'] - df_adx['minus_di']) / dx_denominator_adx.where(dx_denominator_adx != 0, np.nan)
        data['adx'] = df_adx['dx'].ewm(alpha=1/self.adx_period, adjust=False).mean()
        data['plus_di'] = df_adx['plus_di'] # Optional to keep
        data['minus_di'] = df_adx['minus_di'] # Optional to keep
        
        data['obv'] = (np.sign(data['close'].diff()) * data['volume']).fillna(0).cumsum()
        data['atr'] = df_adx['tr'].rolling(window=self.atr_period_risk).mean() # ATR for risk management

        return data.dropna()


    def _get_pivots(self, df, window=5):
        """Helper to find pivot highs and lows. More robust than original update_price_history."""
        pivots = []
        # Simple pivot detection: a point is a pivot high if it's higher than 'window' points on either side.
        for i in range(window, len(df) - window):
            is_high = True
            for j in range(1, window + 1):
                if df['high'].iloc[i] < df['high'].iloc[i-j] or df['high'].iloc[i] < df['high'].iloc[i+j]:
                    is_high = False
                    break
            if is_high:
                pivots.append({'timestamp': df.index[i], 'price': df['high'].iloc[i], 'type': 'high'})

            is_low = True
            for j in range(1, window + 1):
                if df['low'].iloc[i] > df['low'].iloc[i-j] or df['low'].iloc[i] > df['low'].iloc[i+j]:
                    is_low = False
                    break
            if is_low:
                pivots.append({'timestamp': df.index[i], 'price': df['low'].iloc[i], 'type': 'low'})
        
        # Sort pivots by timestamp
        pivots.sort(key=lambda x: x['timestamp'])
        # Remove consecutive pivots of the same type
        unique_pivots = []
        if pivots:
            unique_pivots.append(pivots[0])
            for k in range(1, len(pivots)):
                if pivots[k]['type'] != unique_pivots[-1]['type']:
                    unique_pivots.append(pivots[k])
        return unique_pivots

    def update_price_history(self, df, symbol):
        """Simplified historical swing update, focusing on major swings for context.
           Pattern detection will handle more granular pivots.
        """
        if df is None or df.empty or len(df) < 15: # Need more data for meaningful swings
            logger.debug(f"DataFrame too short for swing update for {symbol}.")
            return
        try:
            # For `price_history['swings']` we can keep the old simple logic or enhance it
            # The `tradingpattern` library will handle its own pivot detection.
            # Here, let's refine the existing simple swing detection slightly.
            df_copy = df.copy()
            window_size = 5 # Check a slightly larger window for more significant local highs/lows
            
            df_copy['is_local_high'] = (df_copy['high'] == df_copy['high'].rolling(window=2*window_size+1, center=True, min_periods=1).max())
            df_copy['is_local_low'] = (df_copy['low'] == df_copy['low'].rolling(window=2*window_size+1, center=True, min_periods=1).min())

            highs = df_copy[df_copy['is_local_high']]
            lows = df_copy[df_copy['is_local_low']]
            
            # This simplified swing logic can be kept for basic context if desired,
            # but complex pattern logic will be separate.
            # For brevity, I'll skip reimplementing the full swing logic from original script here,
            # as the primary pattern detection will be handled by `detect_chart_patterns_enhanced`.
            # The key is to save relevant OHLCV data if you need it for offline ML training feature generation.
            
            # Store historical features for ML training (example)
            # This should be done more systematically if training ML models
            if not df.empty and 'rsi' in df.columns: # Ensure indicators are calculated
                latest_features = df.iloc[-1][['close', 'volume', 'rsi', 'macd', 'adx', 'atr']].to_dict()
                latest_features['timestamp'] = df.index[-1].strftime("%Y-%m-%d %H:%M:%S")
                self.price_history[symbol]["historical_features"].append(latest_features)
                if len(self.price_history[symbol]["historical_features"]) > 5000: # Limit history size
                    self.price_history[symbol]["historical_features"] = self.price_history[symbol]["historical_features"][-5000:]

            self.save_price_history(symbol)
        except Exception as e:
            logger.error(f"Error updating price history for {symbol}: {str(e)}")
            

    def detect_chart_patterns_enhanced(self, df_orig, symbol):
        """Enhanced chart pattern detection using tradingpatternlibrary and custom rules."""
        if df_orig is None or df_orig.empty or len(df_orig) < 30:
            logger.debug(f"Not enough data for enhanced pattern detection for {symbol} ({len(df_orig)} candles).")
            return [], 0 # Return empty list of patterns and neutral signal

        df = df_orig.copy()
        # tradingpatternexpects columns: Date, Open, High, Low, Close, Volume (Date as index is fine)
        # Ensure columns are named as expected if they differ, or adapt the library call.
        # Your DataFrame index is 'timestamp', which should work.

        detected_patterns_info = []
        pattern_signal_score = 0 # Aggregated score from patterns

        try:
            # Using tradingpatternslibrary
            # Note: Ensure your DataFrame has 'Open', 'High', 'Low', 'Close', 'Volume' columns.
            # The library might be sensitive to exact column names.
            # The 'country' parameter is often for stock market data, may not be relevant for crypto.
            
            # The library functions like `find_classic_patterns`, `find_harmonic_patterns` etc.
            # often return a DataFrame with pattern info. We need to adapt.
            # For simplicity, let's focus on a few key patterns that `tradingpattern` might find.
            # `tradingpattern`'s usage is often interactive or for plotting.
            # For programmatic use, you might need to adapt its internal logic or find specific functions.
            # This is a conceptual integration, direct usage of `tradingpattern` for signal generation
            # might require deeper investigation of its API or custom wrappers.

            # Let's define some common patterns it might detect and assign signals
            # This is a simplified way to interact with how such a library *might* be used.
            # The `tradingpattern` library's primary interface `StockMarket(country="US")`
            # is more geared towards analysis than direct signal generation in a loop.
            # For programmatic chart pattern detection, a more direct library or custom implementation
            # of pivot-based logic is often more straightforward.
            
            # Given the complexity of direct `tradingpattern` integration in a loop,
            # let's refine the custom rule-based approach first.
            # The `_get_pivots` method is a starting point.

            pivots = self._get_pivots(df, window=5) # Get sequence of highs and lows
            
            # --- Placeholder for more advanced custom pattern logic ---
            # Example: Detecting a basic Head and Shoulders (very simplified)
            # Requires at least 3 highs (shoulders, head) and 2 lows (neckline points)
            if len(pivots) >= 5:
                # Look for LH - H - RH pattern (Left Shoulder, Head, Right Shoulder)
                for i in range(len(pivots) - 4):
                    if pivots[i]['type'] == 'high' and \
                       pivots[i+1]['type'] == 'low' and \
                       pivots[i+2]['type'] == 'high' and \
                       pivots[i+3]['type'] == 'low' and \
                       pivots[i+4]['type'] == 'high':
                        
                        left_shoulder, L_neck, head, R_neck, right_shoulder = pivots[i:i+5]
                        
                        # Basic H&S conditions
                        if head['price'] > left_shoulder['price'] and head['price'] > right_shoulder['price'] and \
                           abs(left_shoulder['price'] - right_shoulder['price']) / right_shoulder['price'] < 0.03 and \
                           L_neck['price'] < head['price'] and R_neck['price'] < head['price']:
                            
                            neckline_slope = (R_neck['price'] - L_neck['price']) / ((R_neck['timestamp'] - L_neck['timestamp']).total_seconds() if (R_neck['timestamp'] - L_neck['timestamp']).total_seconds() !=0 else 1)
                            # Check if current price broke below neckline
                            # This requires projecting neckline and checking current price against it. More complex.
                            # For simplicity, let's assume a break if close is below min of L_neck and R_neck
                            neckline_low = min(L_neck['price'], R_neck['price'])
                            if df['close'].iloc[-1] < neckline_low:
                                pattern_info = {"name": "Head & Shoulders Top (Simplified)", "signal": -1, "confidence": 0.7, 
                                                "breakout_level": neckline_low, "target": neckline_low - (head['price'] - neckline_low)}
                                detected_patterns_info.append(pattern_info)
                                pattern_signal_score -= 1 # Bearish
                                logger.info(f"{symbol} - Detected: {pattern_info['name']}")
                                break # Process one H&S at a time for now

            # Example: Simplified Ascending Triangle (Bullish Breakout)
            # Needs sequence of higher lows and relatively flat highs
            # This requires more sophisticated trendline fitting and checking.
            # For now, we will keep the original simplified breakout logic as a fallback.

            # --- Original simplified breakout logic (can be a fallback or part of score) ---
            # Ensure we are not in the last candle of the 30-period window to confirm breakout
            if len(df['close']) > 30:
                resistance = df['close'][-31:-1].max() 
                support = df['close'][-31:-1].min() 
                if df['close'].iloc[-1] > resistance and df['volume'].iloc[-1] > df['volume_ma'].iloc[-1] * 1.1: # Added volume confirmation
                    pattern_info = {"name": "Breakout High (30p)", "signal": 1, "confidence": 0.5, "breakout_level": resistance}
                    detected_patterns_info.append(pattern_info)
                    pattern_signal_score += 1 
                    logger.info(f"{symbol} - Detected: {pattern_info['name']}")
                
                elif df['close'].iloc[-1] < support and df['volume'].iloc[-1] > df['volume_ma'].iloc[-1] * 1.1: # Added volume confirmation
                    pattern_info = {"name": "Breakdown Low (30p)", "signal": -1, "confidence": 0.5, "breakout_level": support}
                    detected_patterns_info.append(pattern_info)
                    pattern_signal_score -= 1
                    logger.info(f"{symbol} - Detected: {pattern_info['name']}")
            
            # --- Double Top/Bottom (from original, slightly refined) ---
            if len(df['close']) > 10:
                recent_highs_idx = df['high'][-10:].nlargest(2).index
                recent_lows_idx = df['low'][-10:].nsmallest(2).index

                if len(recent_highs_idx) == 2:
                    h1_val, h2_val = df['high'].loc[recent_highs_idx[0]], df['high'].loc[recent_highs_idx[1]]
                    h1_time, h2_time = recent_highs_idx[0], recent_highs_idx[1]
                    mid_low_val = df['low'].loc[min(h1_time, h2_time):max(h1_time, h2_time)].min()
                    
                    if abs(h1_val - h2_val) / h1_val < 0.015: # Tops within 1.5%
                        if df['close'].iloc[-1] < mid_low_val and df['volume'].iloc[-1] > df['volume_ma'].iloc[-1] * 1.1: # Break below neckline with volume
                            pattern_info = {"name": "Double Top", "signal": -1, "confidence": 0.65, "breakout_level": mid_low_val}
                            detected_patterns_info.append(pattern_info)
                            pattern_signal_score -= 1
                            logger.info(f"{symbol} - Detected: {pattern_info['name']}")

                if len(recent_lows_idx) == 2:
                    l1_val, l2_val = df['low'].loc[recent_lows_idx[0]], df['low'].loc[recent_lows_idx[1]]
                    l1_time, l2_time = recent_lows_idx[0], recent_lows_idx[1]
                    mid_high_val = df['high'].loc[min(l1_time, l2_time):max(l1_time, l2_time)].max()

                    if abs(l1_val - l2_val) / l1_val < 0.015: # Lows within 1.5%
                        if df['close'].iloc[-1] > mid_high_val and df['volume'].iloc[-1] > df['volume_ma'].iloc[-1] * 1.1: # Break above neckline with volume
                            pattern_info = {"name": "Double Bottom", "signal": 1, "confidence": 0.65, "breakout_level": mid_high_val}
                            detected_patterns_info.append(pattern_info)
                            pattern_signal_score += 1
                            logger.info(f"{symbol} - Detected: {pattern_info['name']}")
                                
        except Exception as e:
            logger.error(f"Error in enhanced pattern detection for {symbol}: {e}")
        
        # Clamp pattern_signal_score
        if pattern_signal_score > 1: pattern_signal_score = 1
        if pattern_signal_score < -1: pattern_signal_score = -1
        
        return detected_patterns_info, pattern_signal_score


    def simplified_indicator_signal(self, df, market_regime="neutral"): # Added market_regime
        if df is None or df.empty or len(df) < 2:
            return 0, {} # Return signal and confirmation details
        
        latest = df.iloc[-1]
        prev = df.iloc[-2] if len(df) >=2 else latest # Handle short df
        signal = 0
        confirmations = {"rsi_confirm": False, "macd_confirm": False, "bb_confirm": False, "volume_spike": False}

        # RSI
        if 'rsi' in latest:
            if latest['rsi'] < self.rsi_oversold: 
                signal += 1
                confirmations["rsi_confirm"] = True
            elif latest['rsi'] > self.rsi_overbought: 
                signal -= 1
                confirmations["rsi_confirm"] = True
        
        # MACD Crossover
        if 'macd' in latest and 'macd_signal' in latest and \
           'macd' in prev and 'macd_signal' in prev:
            if (latest['macd'] > latest['macd_signal']) and (prev['macd'] <= prev['macd_signal']): # Bullish crossover
                signal += 1
                confirmations["macd_confirm"] = True
            elif (latest['macd'] < latest['macd_signal']) and (prev['macd'] >= prev['macd_signal']): # Bearish crossover
                signal -= 1
                confirmations["macd_confirm"] = True

        # Bollinger Bands
        if 'close' in latest and 'lower_band' in latest and 'upper_band' in latest:
            if latest['close'] < latest['lower_band']: 
                signal += 1
                confirmations["bb_confirm"] = True
            elif latest['close'] > latest['upper_band']: 
                signal -= 1
                confirmations["bb_confirm"] = True
        
        # Regime-specific logic for indicators
        if self.config.getboolean('STRATEGY', 'REGIME_SPECIFIC_LOGIC', fallback=True):
            if "trending_bullish" in market_regime:
                if signal < 0: signal = 0 # Reduce sell signals in strong bull trend
            elif "trending_bearish" in market_regime:
                if signal > 0: signal = 0 # Reduce buy signals in strong bear trend
            elif "ranging" in market_regime:
                # In ranging, BB signals might be more reliable, MACD crossovers less so
                pass # Example: might give more weight to BB if confirmed

        # Volume Spike (General confirmation, applied after main signals)
        if 'volume' in latest and 'volume_ma' in latest and latest['volume_ma'] > 0:
            if latest['volume'] > latest['volume_ma'] * 1.5: # 50% above MA
                confirmations["volume_spike"] = True
                if signal !=0: # Amplify existing signal
                   signal = signal * 1.5 
        
        # Clamp signal (final decision based on sum of weighted signals)
        if signal >= 1.5: return 1, confirmations # Strong buy
        elif signal <= -1.5: return -1, confirmations # Strong sell
        elif signal >= 0.5: return 0.5, confirmations # Weak buy
        elif signal <= -0.5: return -0.5, confirmations # Weak sell
        else: return 0, confirmations


    # --- ML Scoring Methods ---
    def _load_ml_models(self):
        try:
            model_path = self.config.get('ML_MODELS', 'RELIABILITY_MODEL_PATH', fallback='ml_reliability_model.joblib')
            scaler_path = self.config.get('ML_MODELS', 'RELIABILITY_SCALER_PATH', fallback='ml_reliability_scaler.joblib')
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.ml_reliability_model = joblib.load(model_path)
                self.ml_model_scaler = joblib.load(scaler_path)
                logger.info("Successfully loaded ML reliability model and scaler.")
            else:
                logger.warning("ML reliability model/scaler not found. Scoring will be disabled.")
            
            regime_model_path = self.config.get('ML_MODELS', 'REGIME_MODEL_PATH', fallback='market_regime_model.joblib')
            regime_scaler_path = self.config.get('ML_MODELS', 'REGIME_SCALER_PATH', fallback='market_regime_scaler.joblib')
            if os.path.exists(regime_model_path) and os.path.exists(regime_scaler_path):
                self.market_regime_model = joblib.load(regime_model_path)
                self.market_regime_scaler = joblib.load(regime_scaler_path)
                logger.info("Successfully loaded market regime model and scaler.")
            else:
                logger.warning("Market regime model/scaler not found. Regime detection will be basic.")
        except Exception as e:
            logger.error(f"Error loading ML models: {e}")

    def _prepare_ml_features(self, df_slice, pattern_details_list, indicator_confirmations, market_regime, sentiment_score):
        """Prepares features for the ML reliability model."""
        if df_slice.empty or len(df_slice) < 20: # Need some data for features
            return None 
        
        latest = df_slice.iloc[-1]
        features = {}

        # Basic market features
        features['rsi'] = latest.get('rsi', 50)
        features['macd_hist'] = latest.get('macd_hist', 0)
        features['adx'] = latest.get('adx', 20)
        features['volume_change_pct'] = latest.get('volume_change_pct', 0)
        features['atr_pct'] = (latest.get('atr', 0) / latest.get('close', 1)) * 100 # ATR as % of price
        features['stoch_k'] = latest.get('stoch_k', 50)
        
        # Bollinger Band features
        if 'close' in latest and 'upper_band' in latest and 'lower_band' in latest and (latest['upper_band'] - latest['lower_band']) != 0 :
            features['bb_width'] = (latest['upper_band'] - latest['lower_band']) / latest['sma_bb'] if latest.get('sma_bb',0) != 0 else 0
            features['bb_pos'] = (latest['close'] - latest['lower_band']) / (latest['upper_band'] - latest['lower_band']) if (latest['upper_band'] - latest['lower_band']) != 0 else 0.5
        else:
            features['bb_width'] = 0
            features['bb_pos'] = 0.5


        # Pattern features (use the first detected strong pattern or default)
        # This needs to be standardized. For example, use one-hot encoding for pattern names.
        # For simplicity here, let's just use the 'signal' from the first pattern if any.
        primary_pattern_signal = 0
        primary_pattern_confidence = 0
        if pattern_details_list:
            primary_pattern_signal = pattern_details_list[0].get('signal',0)
            primary_pattern_confidence = pattern_details_list[0].get('confidence', 0) # If patterns provide confidence
        features['pattern_signal'] = primary_pattern_signal 
        features['pattern_confidence'] = primary_pattern_confidence # Requires patterns to output this

        # Indicator confirmation features (boolean to int)
        features['rsi_confirmed'] = 1 if indicator_confirmations.get('rsi_confirm') else 0
        features['macd_confirmed'] = 1 if indicator_confirmations.get('macd_confirm') else 0
        features['bb_confirmed'] = 1 if indicator_confirmations.get('bb_confirm') else 0
        features['volume_spike_confirmed'] = 1 if indicator_confirmations.get('volume_spike') else 0
        
        # Market Regime (one-hot encode this if you have defined categories)
        # Example: if regimes are 0, 1, 2 -> feature_regime_0, feature_regime_1, etc.
        # For now, just a numeric representation or a simple mapping.
        regime_map = {"neutral":0, "trending_bullish":1, "trending_bearish":-1, "ranging_high_vol":2, "ranging_low_vol":-2}
        features['market_regime_numeric'] = regime_map.get(market_regime.split("_")[0] if "_" in market_regime else market_regime, 0)


        # Sentiment Score
        features['sentiment_score'] = sentiment_score if sentiment_score is not None else 0

        # Ensure all features are present and in correct order for the model
        # This order must match the order used during model training
        # Example feature order:
        expected_feature_order = [
            'rsi', 'macd_hist', 'adx', 'volume_change_pct', 'atr_pct', 'stoch_k', 'bb_width', 'bb_pos',
            'pattern_signal', 'pattern_confidence', 'rsi_confirmed', 'macd_confirmed', 'bb_confirmed', 
            'volume_spike_confirmed', 'market_regime_numeric', 'sentiment_score'
        ]
        
        # Create a Pandas Series with the defined order, filling missing ones with 0 or a neutral value
        feature_series = pd.Series(index=expected_feature_order, dtype=float)
        for f_name in expected_feature_order:
            feature_series[f_name] = features.get(f_name, 0) # Default to 0 if a feature is missing

        return feature_series.values.reshape(1, -1) # Return as 2D array for scaler/model

    def score_signal_reliability_ml(self, df_slice, pattern_details_list, indicator_confirmations, market_regime, sentiment_score, direction_to_score):
        """Scores a potential signal using a pre-trained ML model."""
        if not self.config.getboolean('STRATEGY','ENABLE_ML_SCORING', fallback=False) or self.ml_reliability_model is None or self.ml_model_scaler is None:
            # If ML is disabled or model not loaded, return a default high confidence for any valid signal
            # Or, you could return a neutral score (e.g., 0.5) and let other logic decide.
            # For now, let's say it doesn't modify the signal strength if ML is off.
            logger.debug("ML scoring disabled or model not loaded. Passing through.")
            return 1.0 # Default: no ML opinion, rely on rule-based

        features_array = self._prepare_ml_features(df_slice, pattern_details_list, indicator_confirmations, market_regime, sentiment_score)
        if features_array is None:
            logger.warning("Could not prepare features for ML scoring.")
            return 0.0 # Low confidence if features cannot be prepared

        try:
            scaled_features = self.ml_model_scaler.transform(features_array)
            # Model should output probabilities for classes (e.g., class 0: sell, class 1: neutral, class 2: buy)
            # Or (class 0: no success, class 1: success)
            # This depends ENTIRELY on how your model was trained.
            # Assuming model.predict_proba gives [[P(class_0), P(class_1), P(class_2)]]
            # And class mapping: 0 -> Sell_Success, 1 -> Buy_Success (or however you trained it)
            
            # EXAMPLE: If your model predicts [prob_of_no_success, prob_of_success]
            # proba = self.ml_reliability_model.predict_proba(scaled_features)[0]
            # reliability_score = proba[1] # Probability of success

            # EXAMPLE: If your model predicts [P(Sell_Success), P(Neutral), P(Buy_Success)]
            # And your classes_ attribute is like ['sell_success', 'neutral', 'buy_success']
            # class_mapping = {label: idx for idx, label in enumerate(self.ml_reliability_model.classes_)}
            # proba = self.ml_reliability_model.predict_proba(scaled_features)[0]
            # if direction_to_score == 1: # Long
            #    reliability_score = proba[class_mapping.get('buy_success', -1)] if 'buy_success' in class_mapping else proba[-1] # Default to last class if mapping fails
            # elif direction_to_score == -1: # Short
            #    reliability_score = proba[class_mapping.get('sell_success', -1)] if 'sell_success' in class_mapping else proba[0] # Default to first class
            # else:
            #    reliability_score = 0.0
            
            # For a generic approach, if you train a model to predict if a "signal of given direction" will succeed:
            # The features should implicitly include the proposed direction or model should be direction-agnostic quality score.
            # Let's assume the model gives a general quality score (0 to 1) for the *current conditions*
            # This is a simplification. True ML scoring is more nuanced.
            # For now, let's assume it's a binary classifier: 0=bad_trade, 1=good_trade
            # and predict_proba gives [[P(bad), P(good)]]
            proba = self.ml_reliability_model.predict_proba(scaled_features)[0]
            reliability_score = proba[1] # Probability of "good trade" based on current features
            
            logger.info(f"ML Reliability Score: {reliability_score:.3f} (Probas: {proba})")
            return reliability_score
        except Exception as e:
            logger.error(f"Error during ML reliability scoring: {e}")
            return 0.0 # Low confidence on error

    # --- Market Regime Detection Methods ---
    def _get_regime_features(self, df):
        """Prepares features for market regime detection model."""
        if df.empty or len(df) < 30: # Need some data
            return None
        
        latest = df.iloc[-1]
        # Features for regime: Volatility (ATR %), Trend Strength (ADX)
        # Could also use: Std Dev of returns, Price distance from long-term MA, etc.
        atr_pct = (latest.get('atr',0) / latest.get('close',1)) * 100 if latest.get('close') else 0
        adx = latest.get('adx', 25) # Default to 25 if not available (weak trend / ranging)
        
        # Example: Price distance from a long MA (e.g., 50-period EMA)
        # data['ema_very_long'] = data['close'].ewm(span=50, adjust=False).mean()
        # price_dist_ema50_pct = (latest.get('close',0) - latest.get('ema_very_long', latest.get('close',0))) / latest.get('close',1) * 100
        
        # This order must match training
        feature_values = [atr_pct, adx] 
        return np.array(feature_values).reshape(1, -1)

    def detect_market_regime(self, df, symbol):
        """Detects market regime using a pre-trained K-Means model or rules."""
        if not self.config.getboolean('STRATEGY','ENABLE_MARKET_REGIME', fallback=False) :
             return "neutral_default" # Default if disabled

        if self.market_regime_model and self.market_regime_scaler:
            features_array = self._get_regime_features(df)
            if features_array is None:
                logger.warning(f"Could not prepare features for market regime detection for {symbol}.")
                return "neutral_error_prep"
            try:
                scaled_features = self.market_regime_scaler.transform(features_array)
                regime_label = self.market_regime_model.predict(scaled_features)[0]
                # Map numeric label back to a meaningful string
                # This mapping depends on how you interpret clusters during/after training.
                # E.g., cluster 0 = "ranging_low_vol", cluster 1 = "trending_bullish_high_vol", etc.
                # This needs to be defined by you after inspecting the clusters from training.
                # Placeholder mapping:
                regime_map_ml = {
                    0: "ml_regime_0_custom", 1: "ml_regime_1_custom", 
                    2: "ml_regime_2_custom", 3: "ml_regime_3_custom" 
                    # Add more if your K-Means has more clusters
                }
                regime_str = regime_map_ml.get(regime_label, f"ml_regime_unknown_{regime_label}")
                logger.info(f"{symbol} - ML Detected Market Regime: {regime_str} (Label: {regime_label})")
                return regime_str
            except Exception as e:
                logger.error(f"Error during ML market regime detection for {symbol}: {e}")
                # Fallback to rule-based if ML fails
        
        # Fallback Rule-based regime detection if ML model not loaded or fails
        if df.empty or len(df) < self.adx_period:
            return "neutral_no_data"
        
        latest = df.iloc[-1]
        adx = latest.get('adx', 25)
        plus_di = latest.get('plus_di', 0)
        minus_di = latest.get('minus_di', 0)
        atr_pct = (latest.get('atr', 0) / latest.get('close', 1)) * 100 if latest.get('close') else 0

        volatility_threshold_high = 2.5 # ATR %
        volatility_threshold_low = 0.8  # ATR %
        adx_trend_threshold = 25
        adx_strong_trend_threshold = 40

        regime = "neutral_rule_based"
        if adx > adx_strong_trend_threshold:
            if plus_di > minus_di: regime = "strong_trending_bullish"
            else: regime = "strong_trending_bearish"
        elif adx > adx_trend_threshold:
            if plus_di > minus_di: regime = "trending_bullish"
            else: regime = "trending_bearish"
        else: # ADX < trend_threshold (ranging)
            if atr_pct > volatility_threshold_high: regime = "ranging_high_vol"
            elif atr_pct < volatility_threshold_low: regime = "ranging_low_vol"
            else: regime = "ranging_mid_vol"
        
        logger.info(f"{symbol} - Rule-based Market Regime: {regime} (ADX: {adx:.2f}, ATR%: {atr_pct:.2f}%)")
        return regime

    # --- Sentiment Analysis Methods ---
    def _clean_text_for_sentiment(self, text):
        tokens = word_tokenize(text.lower())
        return " ".join([word for word in tokens if word.isalpha() and word not in self.stop_words])

    def _fetch_generic_crypto_news_example(self, symbol_query):
        """Very basic example: Scrapes Google News. NOT ROBUST OR RECOMMENDED FOR LIVE TRADING."""
        # This is highly unreliable and against ToS for many sites.
        # Use proper News APIs (e.g., newsapi.org, mediastack, cryptopanic) for real use.
        logger.warning("Using highly unreliable generic news scraping for sentiment. For real use, integrate a proper News API.")
        headers = {"User-Agent": "Mozilla/5.0"} # Basic user agent
        # Construct a search query, e.g., for "BTC" from "BTC/USDT"
        query_term = symbol_query.split('/')[0] if '/' in symbol_query else symbol_query
        # Search for "<coin> crypto news"
        try:
            # This is a placeholder for a more robust news fetching mechanism
            # response = requests.get(f"https://news.google.com/search?q={query_term}%20cryptocurrency&hl=en-US&gl=US&ceid=US%3Aen", headers=headers, timeout=10)
            # response.raise_for_status()
            # soup = BeautifulSoup(response.text, 'html.parser')
            # headlines = [a.text for a in soup.find_all('h3', {'class': 'ipQwMb ekueJc RD0gLbà¸„à¸“à¸°à¸à¸£à¸£à¸¡à¸à¸²à¸£à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸•à¸±à¹‰à¸‡'})[:5]] # Example class, will break
            
            # Fallback if scraping fails or is disabled
            logger.info(f"Generic news scraping example called for {query_term}. Returning placeholder text.")
            headlines = [f"{query_term} price shows volatility amidst market uncertainty.",
                         f"Experts discuss future of {query_term} adoption.",
                         f"Regulatory news impacts {query_term} outlook."] # Placeholder headlines
            return headlines
        except Exception as e:
            logger.error(f"Error in basic news scraping example for {symbol_query}: {e}")
            return []


    def get_sentiment_score(self, symbol):
        """
        Fetches and analyzes sentiment for a symbol.
        Placeholder: Uses VADER on generic example headlines. Needs real news feed.
        Returns a compound sentiment score (-1 to 1).
        """
        if not self.config.getboolean('STRATEGY','ENABLE_SENTIMENT_ANALYSIS', fallback=False):
            return 0.0 # Neutral if disabled

        sentiment_source = self.config.get('SENTIMENT', 'SENTIMENT_DATA_SOURCE', fallback='VADER_GENERIC')
        
        headlines = []
        if sentiment_source == 'NEWS_API':
            # TODO: Implement actual News API fetching logic here
            # news_api_key = self.config.get('SENTIMENT','NEWS_API_KEY', fallback=None)
            # if news_api_key and news_api_key != 'YOUR_NEWS_API_KEY':
            #    # headlines = self._fetch_from_news_api(symbol, news_api_key) # You'd build this method
            #    pass
            logger.warning(f"NEWS_API sentiment source selected for {symbol} but not implemented. Using generic.")
            headlines = self._fetch_generic_crypto_news_example(symbol) # Fallback for now
        elif sentiment_source == 'VADER_GENERIC' or not headlines: # Fallback
             headlines = self._fetch_generic_crypto_news_example(symbol)

        if not headlines:
            logger.info(f"No headlines found for sentiment analysis for {symbol}.")
            return 0.0

        compound_scores = []
        for headline in headlines:
            cleaned_headline = self._clean_text_for_sentiment(headline)
            if cleaned_headline:
                vs = self.sentiment_analyzer.polarity_scores(cleaned_headline)
                compound_scores.append(vs['compound'])
        
        if not compound_scores:
            logger.info(f"No valid compound sentiment scores derived for {symbol}.")
            return 0.0
        
        avg_compound_score = np.mean(compound_scores)
        logger.info(f"{symbol} - Avg Sentiment Score ({len(compound_scores)} headlines): {avg_compound_score:.3f} ({sentiment_source})")
        return avg_compound_score


    # --- Enhanced Risk Management ---
    def calculate_dynamic_tp_sl_enhanced(self, df, entry_price, direction, symbol, market_regime="neutral", signal_confidence=1.0):
        if df is None or df.empty or 'atr' not in df.columns or df['atr'].isnull().all():
            logger.warning(f"{symbol} - ATR not available for dynamic TP/SL. Using fallback percentages.")
            # This fallback needs to be defined or use fixed $ amounts, simple % is risky.
            # For now, let's ensure it's very conservative if ATR fails.
            sl_offset = entry_price * (self.config.getfloat('RISK', 'FALLBACK_SL_PCT', fallback=0.01)) # 1% fallback SL
            tp_offset = entry_price * (self.config.getfloat('RISK', 'FALLBACK_TP_PCT', fallback=0.015)) # 1.5% fallback TP
        else:
            atr = df['atr'].iloc[-1]
            if atr == 0 or pd.isna(atr): 
                atr = entry_price * 0.01 # Safety for zero/NaN ATR (1% of price)
                logger.warning(f"{symbol} - ATR was zero or NaN, using 1% of entry price as ATR substitute.")

            sl_atr_mult = self.default_stop_loss_atr_multiplier
            tp1_atr_mult = self.default_tp_atr_multiplier
            
            # Adjust ATR multipliers by regime (example)
            if self.config.getboolean('STRATEGY', 'REGIME_SPECIFIC_LOGIC', fallback=True):
                if "ranging_high_vol" in market_regime or "strong_trending" in market_regime :
                    sl_atr_mult *= 1.2 # Wider SL in high vol or strong trends
                    tp1_atr_mult *= 1.2 # Wider TP
                elif "ranging_low_vol" in market_regime:
                    sl_atr_mult *= 0.8 # Tighter SL in low vol
                    tp1_atr_mult *= 0.8

            # Adjust by ML signal confidence (example: 0.5 to 1.5 multiplier range)
            # Confidence affects how aggressive the SL/TP is. Higher confidence might allow wider SL or more aggressive TP.
            # This logic needs careful thought. For now, let's say higher confidence = slightly tighter SL, more aggressive TP
            confidence_factor_sl = 1.0 + (0.5 * (1.0 - signal_confidence)) # If confidence is 1, factor is 1. If 0.5, factor is 1.25 (wider SL)
            confidence_factor_tp = 1.0 + (0.5 * signal_confidence) # If confidence is 1, factor is 1.5. If 0.5, factor is 1.25
            
            sl_atr_mult *= confidence_factor_sl
            tp1_atr_mult *= confidence_factor_tp
            
            sl_offset = atr * sl_atr_mult
            tp_offset = atr * tp1_atr_mult


        if direction == 1: # Long
            SL = entry_price - sl_offset
            TP1 = entry_price + tp_offset
            TP2 = entry_price + (tp_offset / self.default_tp_atr_multiplier * self.tp2_atr_multiplier) # Scale TP2, TP3 based on TP1's ATR multiple
            TP3 = entry_price + (tp_offset / self.default_tp_atr_multiplier * self.tp3_atr_multiplier)
        else: # Short
            SL = entry_price + sl_offset
            TP1 = entry_price - tp_offset
            TP2 = entry_price - (tp_offset / self.default_tp_atr_multiplier * self.tp2_atr_multiplier)
            TP3 = entry_price - (tp_offset / self.default_tp_atr_multiplier * self.tp3_atr_multiplier)
            
        # Ensure SL and TPs are reasonably distanced from entry, prevent crossing over
        if direction == 1:
            SL = min(SL, entry_price * (1 - self.config.getfloat('RISK','MIN_SL_PCT_FROM_ENTRY', fallback=0.002))) # Min 0.2% away
            TP1 = max(TP1, entry_price * (1 + self.config.getfloat('RISK','MIN_TP_PCT_FROM_ENTRY', fallback=0.003))) # Min 0.3% away
            if TP1 <= SL : TP1 = entry_price + (entry_price - SL) # Ensure TP is beyond SL
        else: #direction == -1
            SL = max(SL, entry_price * (1 + self.config.getfloat('RISK','MIN_SL_PCT_FROM_ENTRY', fallback=0.002)))
            TP1 = min(TP1, entry_price * (1 - self.config.getfloat('RISK','MIN_TP_PCT_FROM_ENTRY', fallback=0.003)))
            if TP1 >= SL : TP1 = entry_price - (SL - entry_price)
            
        # Ensure TP2 > TP1 (for long) or TP2 < TP1 (for short), etc.
        if direction == 1:
            TP2 = max(TP2, TP1 * 1.001) # Ensure TP2 is slightly beyond TP1
            TP3 = max(TP3, TP2 * 1.001) # Ensure TP3 is slightly beyond TP2
        else:
            TP2 = min(TP2, TP1 * 0.999)
            TP3 = min(TP3, TP2 * 0.999)

        # Round to appropriate precision for the symbol (fetch from exchange market data if needed)
        # For now, using a generic rounding, e.g. 4-6 decimal places for USDT pairs
        price_precision = self.exchange.markets[symbol]['precision']['price'] if symbol in self.exchange.markets else 4 # Default
        
        return (round(TP1, price_precision), round(TP2, price_precision), 
                round(TP3, price_precision), round(SL, price_precision))

    def _calculate_position_size(self, symbol, entry_price, stop_loss_price):
        """Calculates position size based on fixed fractional risk."""
        try:
            balance_info = self.exchange.fetch_balance()
            usdt_balance = balance_info['USDT']['free'] if 'USDT' in balance_info and 'free' in balance_info['USDT'] else 1000 # Fallback
        except Exception as e:
            logger.error(f"Could not fetch USDT balance for position sizing: {e}. Using fallback $1000.")
            usdt_balance = 1000

        risk_amount_usdt = usdt_balance * self.risk_per_trade_pct
        price_per_unit_sl = abs(entry_price - stop_loss_price)

        if price_per_unit_sl == 0:
            logger.warning(f"Stop loss price is same as entry price for {symbol}. Cannot calculate position size. Defaulting small.")
            return 0.01 # Minimal size

        position_size_asset = risk_amount_usdt / price_per_unit_sl
        
        # Adjust for minimum order size and precision for the asset
        market = self.exchange.markets.get(symbol)
        if market:
            min_amount = market.get('limits', {}).get('amount', {}).get('min')
            amount_precision = market.get('precision', {}).get('amount')

            if min_amount is not None and position_size_asset < min_amount:
                logger.warning(f"Calculated position size {position_size_asset} for {symbol} is below minimum {min_amount}. Adjusting or skipping.")
                # Could skip or set to min_amount if balance allows that risk. For now, let's assume we might skip if too small.
                # To be conservative, if calculated size is below min, it means risk per trade for min qty is too high.
                return 0 # Indicate can't meet min size with current risk

            if amount_precision is not None:
                # ccxt.decimal_to_precision can format this
                position_size_asset = float(self.exchange.amount_to_precision(symbol, position_size_asset))

        logger.info(f"Calculated position size for {symbol}: {position_size_asset} units. Risking ${risk_amount_usdt:.2f} (USDT Balance: ${usdt_balance:.2f})")
        return position_size_asset

    def generate_final_trade_signal(self, symbol: str):
        data_all_tf = {}
        # Fetch primary TF first for latest context
        df_primary = self.fetch_ohlcv_data(symbol, timeframe=self.default_ohlcv_timeframe, limit=self.config.getint('TRADING','MIN_CANDLES_FOR_SIGNAL',fallback=100))
        if df_primary.empty or len(df_primary) < self.config.getint('TRADING','MIN_CANDLES_FOR_SIGNAL',fallback=50):
            logger.warning(f"Not enough primary TF data for {symbol} ({len(df_primary)} candles).")
            return None
        
        df_primary_indic = self.calculate_indicators(df_primary)
        if df_primary_indic.empty:
            logger.warning(f"Could not calculate indicators for primary TF for {symbol}.")
            return None
        data_all_tf[self.default_ohlcv_timeframe] = df_primary_indic

        # Fetch other timeframes
        for tf in self.timeframes:
            if tf == self.default_ohlcv_timeframe:
                continue
            df_tf_data = self.fetch_ohlcv_data(symbol, timeframe=tf, limit=200) # Keep 200 for other TFs context
            if not df_tf_data.empty:
                df_tf_indic = self.calculate_indicators(df_tf_data)
                if not df_tf_indic.empty:
                    data_all_tf[tf] = df_tf_indic
        
        # --- 1. Market Regime Detection ---
        market_regime = self.detect_market_regime(df_primary_indic, symbol) # Use primary TF for current regime

        # --- 2. Sentiment Analysis ---
        sentiment_score = self.get_sentiment_score(symbol)

        # --- 3. Signal Aggregation from Timeframes ---
        timeframe_signal_strengths = [] # Store (signal_value, confidence/weight)
        all_pattern_details = [] # Collect all pattern details from all TFs
        all_indicator_confirmations = {} # Collect indicator confirmations from primary TF

        for tf_name, df_tf in data_all_tf.items():
            if df_tf.empty: continue

            # Get indicator-based signal & confirmations
            indicator_raw_signal, indicator_confirms = self.simplified_indicator_signal(df_tf, market_regime)
            if tf_name == self.default_ohlcv_timeframe: # Store primary TF confirmations
                all_indicator_confirmations = indicator_confirms

            # Get pattern-based signal
            patterns_found_tf, pattern_raw_signal = [], 0
            if self.config.getboolean('STRATEGY','ENABLE_CHART_PATTERNS', fallback=True):
                 patterns_found_tf, pattern_raw_signal = self.detect_chart_patterns_enhanced(df_tf, symbol)
                 if patterns_found_tf: all_pattern_details.extend(patterns_found_tf)
            
            # Combine raw signals for this timeframe (simple sum for now, could be weighted)
            combined_tf_signal = indicator_raw_signal + pattern_raw_signal
            
            # Weight based on timeframe (e.g., primary TF more weight)
            weight = 1.0
            if tf_name == self.default_ohlcv_timeframe: weight = 1.5
            elif tf_name == self.timeframes[1]: weight = 1.0 # e.g. 4h
            else: weight = 0.75 # e.g. 1d
            
            timeframe_signal_strengths.append(combined_tf_signal * weight)

        if not timeframe_signal_strengths:
            logger.warning(f"No valid timeframe signals for {symbol}.")
            return None
            
        aggregated_signal_strength = sum(timeframe_signal_strengths)
        
        # Determine initial direction based on aggregated rule-based signals
        rule_based_direction = 0
        # Adjust threshold based on number of TFs to avoid over-sensitivity
        # Example: If 3 TFs, max possible score (1.5*2 + 1*2 + 0.75*2) = 3+2+1.5 = 6.5
        # Let's say a threshold of 2.0 is strong enough for an initial direction
        signal_threshold = 2.0 
        if aggregated_signal_strength >= signal_threshold: rule_based_direction = 1  # Long
        elif aggregated_signal_strength <= -signal_threshold: rule_based_direction = -1 # Short
        
        if rule_based_direction == 0:
            logger.info(f"{symbol} - No strong rule-based consensus (Agg Strength: {aggregated_signal_strength:.2f}).")
            return None

        # --- 4. ML Reliability Scoring for the proposed rule_based_direction ---
        ml_score = self.score_signal_reliability_ml(
            df_primary_indic, all_pattern_details, all_indicator_confirmations, 
            market_regime, sentiment_score, rule_based_direction
        )
        
        final_confidence = aggregated_signal_strength * ml_score # Combine rule strength with ML confidence
        
        # --- 5. Sentiment Overlay / Filter ---
        if self.config.getboolean('STRATEGY','ENABLE_SENTIMENT_ANALYSIS', fallback=False):
            if rule_based_direction == 1 and sentiment_score < self.min_sentiment_score_long:
                logger.info(f"{symbol} - Long signal filtered out by negative/neutral sentiment ({sentiment_score:.2f}).")
                return None
            if rule_based_direction == -1 and sentiment_score > self.max_sentiment_score_short:
                logger.info(f"{symbol} - Short signal filtered out by positive/neutral sentiment ({sentiment_score:.2f}).")
                return None
        
        # --- 6. Final Decision based on ML score and other factors ---
        if self.config.getboolean('STRATEGY','ENABLE_ML_SCORING', fallback=False) and ml_score < self.min_pattern_reliability_score:
            logger.info(f"{symbol} - Signal ({'Long' if rule_based_direction ==1 else 'Short'}) filtered out by low ML score ({ml_score:.2f}). Required: {self.min_pattern_reliability_score}")
            return None

        # If ML is not enabled, we rely on the rule_based_direction and its strength
        if not self.config.getboolean('STRATEGY','ENABLE_ML_SCORING', fallback=False) and rule_based_direction == 0:
             return None # Already handled, but for clarity

        entry_price = df_primary_indic['close'].iloc[-1]
        TP1, TP2, TP3, SL = self.calculate_dynamic_tp_sl_enhanced(
            df_primary_indic, entry_price, rule_based_direction, symbol, market_regime, ml_score # Pass ML score as confidence
        )

        # Position Sizing
        position_size = self._calculate_position_size(symbol, entry_price, SL)
        if position_size == 0: # Cannot meet min size for risk
            logger.warning(f"{symbol} - Skipping trade due to position size constraints for defined risk.")
            return None
            
        current_active_trade_count = sum(len(trades) for trades in self.active_trades.values())
        if current_active_trade_count >= self.max_concurrent_trades:
            logger.warning(f"Max concurrent trades ({self.max_concurrent_trades}) reached. Skipping new signal for {symbol}.")
            return None


        return {
            'symbol': symbol,
            'direction': 'Long' if rule_based_direction == 1 else 'Short',
            'entry_price': entry_price,
            'TP1': TP1, 'TP2': TP2, 'TP3': TP3, 'SL': SL,
            'position_size': position_size, # Added
            'market_regime': market_regime,
            'sentiment_score': sentiment_score,
            'ml_reliability_score': ml_score,
            'rule_based_signal_strength': aggregated_signal_strength,
            'final_confidence_score': final_confidence, # Optional combined score
            'pattern_details': all_pattern_details[:3] # Top 3 patterns for info
        }

    def get_market_context(self, df, symbol): # Already quite good
        if df is None or df.empty or len(df) < 2:
            return {
                "symbol": symbol, "price": np.nan, "price_change_24h": 0, "volatility_atr_pct": 0,
                "rsi": np.nan, "trend_ema": "N/A", "bollinger_position": "N/A", "adx": np.nan,
                "current_regime": "Unknown" # Add regime here
            }
        last_row = df.iloc[-1]
        price = last_row['close']
        
        candles_for_24h = 24 if self.default_ohlcv_timeframe == '1h' else (6 if self.default_ohlcv_timeframe == '4h' else 1)
        price_change_24h = 0
        if len(df) >= candles_for_24h:
            price_24h_ago = df['close'].iloc[-candles_for_24h]
            price_change_24h = ((price - price_24h_ago) / price_24h_ago) * 100 if price_24h_ago != 0 else 0
        
        atr = last_row.get('atr',0)
        volatility_atr_pct = (atr / price) * 100 if price != 0 else 0
        rsi_val = last_row.get('rsi', np.nan)
        adx_val = last_row.get('adx', np.nan)
        
        trend_ema = "N/A"
        if 'ema_short' in last_row and 'ema_long' in last_row and \
           not pd.isna(last_row['ema_short']) and not pd.isna(last_row['ema_long']):
            if last_row['ema_short'] > last_row['ema_long']: trend_ema = "Bullish"
            else: trend_ema = "Bearish"
        
        bb_position = "N/A"
        if 'upper_band' in last_row and 'lower_band' in last_row and \
            not pd.isna(last_row['upper_band']) and not pd.isna(last_row['lower_band']):
            if price > last_row['upper_band']: bb_position = "Above upper band"
            elif price < last_row['lower_band']: bb_position = "Below lower band"
            else: bb_position = "Between bands"

        current_regime = self.detect_market_regime(df, symbol) # Get current regime
        
        return {
            "symbol": symbol, "price": price, "price_change_24h": price_change_24h,
            "volatility_atr_pct": volatility_atr_pct, "rsi": rsi_val, "trend_ema": trend_ema, 
            "bollinger_position": bb_position, "adx": adx_val, "current_regime": current_regime
        }

    def backtest_strategy(self, symbol, days=30): # Increased default backtest days
        # ... (Your existing backtest_strategy structure) ...
        # CRITICAL: This method needs to be updated to use the new
        # `generate_final_trade_signal` logic for consistency.
        # This involves:
        # 1. Simulating the multi-timeframe data fetching for each historical point.
        # 2. Calling the enhanced `generate_final_trade_signal` which includes
        #    pattern detection, regime, sentiment (mocked for backtest), ML scoring (mocked).
        # 3. Simulating position sizing, partial TPs, and trailing stops.
        # This is a significant refactor of the backtester.
        # For now, I will leave the core loop but you MUST update it.
        logger.warning(f"BACKTESTER FOR {symbol} NEEDS SIGNIFICANT UPDATES to reflect new signal generation & risk management.")
        
        # --- Simplified Backtest Loop (NEEDS FULL REWORK) ---
        default_timeframe_for_backtest = self.default_ohlcv_timeframe
        # Calculate how many candles are in `days` for the given timeframe
        tf_minutes = self._timeframe_to_minutes(default_timeframe_for_backtest)
        candles_needed = int((days * 24 * 60) / tf_minutes) + 200 # Buffer for longest indicator

        df_hist_full = self.fetch_ohlcv_data(symbol, timeframe=default_timeframe_for_backtest, limit=candles_needed)
        if df_hist_full.empty or len(df_hist_full) < candles_needed * 0.8: # Need most of the data
            logger.warning(f"Not enough historical data for backtesting {symbol} ({len(df_hist_full)} candles).")
            return {'symbol': symbol, 'period': f"{days} days", 'total_trades': 0, 'message': 'Not enough historical data'}, []

        # For a proper backtest, you'd iterate through `df_hist_full` candle by candle.
        # At each candle, you'd take `df_hist_full.iloc[:current_index+1]` as the "current available data".
        # Then, you would simulate `fetch_all_timeframes` using resampled data from this historical slice.
        # And call `generate_final_trade_signal` with that.
        # This is complex. Below is a highly simplified placeholder.

        # Placeholder: Simulate signals on the full dataframe (not realistic for point-in-time backtest)
        # This simplified version WILL cause lookahead bias if not careful.
        # df_with_indicators = self.calculate_indicators(df_hist_full) # Do this once
        # if df_with_indicators.empty: return {'message':'No indicators'}, []

        trades = []
        balance = 1000
        initial_balance = 1000
        active_trade_details = None # Stores current open trade details

        # Iterating candle by candle for a more realistic (though still simplified) backtest
        for i in range(200, len(df_hist_full)): # Start after indicator warmup
            current_df_slice_primary = df_hist_full.iloc[:i+1] # Data available up to this point for primary TF
            
            # --- Mocking multi-timeframe data for backtest ---
            # This is a simplification. A full backtester would resample historical data.
            # For this example, we'll just use the primary timeframe data to generate signals.
            # This means the backtest won't fully reflect the multi-TF logic of live trading.
            mock_data_all_tf_backtest = {
                self.default_ohlcv_timeframe: self.calculate_indicators(current_df_slice_primary)
            }
            # You would need to properly resample `current_df_slice_primary` to other TFs for a true backtest.

            # --- Simulate generate_final_trade_signal for backtesting ---
            # This is where the main logic of signal generation is called.
            # We need to adapt the `generate_final_trade_signal` or create a backtest version.
            # For simplicity, let's assume a simplified signal generation for backtest or that
            # `generate_final_trade_signal` can be adapted to take historical `data_all_tf`.
            
            # This is a placeholder. You need to integrate the full signal logic here.
            # For this pass, let's just use a very basic indicator cross on the primary TF
            # to simulate signal generation.
            # THIS IS NOT THE FULL BOT LOGIC.
            temp_df = mock_data_all_tf_backtest.get(self.default_ohlcv_timeframe)
            if temp_df is None or len(temp_df) < 2: continue
            
            simulated_signal_direction = 0
            if temp_df['ema_short'].iloc[-1] > temp_df['ema_long'].iloc[-1] and \
               temp_df['ema_short'].iloc[-2] < temp_df['ema_long'].iloc[-2]:
                simulated_signal_direction = 1 # Buy
            elif temp_df['ema_short'].iloc[-1] < temp_df['ema_long'].iloc[-1] and \
                 temp_df['ema_short'].iloc[-2] > temp_df['ema_long'].iloc[-2]:
                simulated_signal_direction = -1 # Sell

            current_price = temp_df['close'].iloc[-1]
            current_time = temp_df.index[-1]

            # Manage existing trade (simplified SL/TP check)
            if active_trade_details:
                pnl_pct = 0
                closed = False
                if active_trade_details['direction'] == 1: # Long
                    if current_price <= active_trade_details['sl']:
                        pnl_pct = (active_trade_details['sl'] - active_trade_details['entry']) / active_trade_details['entry']
                        closed = True; status = "SL"
                    elif current_price >= active_trade_details['tp1']:
                        pnl_pct = (active_trade_details['tp1'] - active_trade_details['entry']) / active_trade_details['entry']
                        closed = True; status = "TP1"
                else: # Short
                    if current_price >= active_trade_details['sl']:
                        pnl_pct = (active_trade_details['entry'] - active_trade_details['sl']) / active_trade_details['entry']
                        closed = True; status = "SL"
                    elif current_price <= active_trade_details['tp1']:
                        pnl_pct = (active_trade_details['entry'] - active_trade_details['tp1']) / active_trade_details['entry']
                        closed = True; status = "TP1"
                
                if closed:
                    balance *= (1 + pnl_pct) # No leverage, simple PNL
                    trades.append({
                        'entry_time': active_trade_details['entry_time'], 'entry_price': active_trade_details['entry'],
                        'exit_time': current_time, 'exit_price': current_price if status == "SL" else active_trade_details['tp1'], # Price at exit
                        'profit_pct': pnl_pct * 100, 'status': status
                    })
                    active_trade_details = None
            
            # Enter new trade
            if not active_trade_details and simulated_signal_direction != 0:
                entry_price = current_price
                # Use the full slice for ATR calculation for this point in time
                tp1_b, _, _, sl_b = self.calculate_dynamic_tp_sl_enhanced(temp_df, entry_price, simulated_signal_direction, symbol, "backtest_neutral")
                
                active_trade_details = {
                    'entry_time': current_time,
                    'entry': entry_price,
                    'direction': simulated_signal_direction,
                    'sl': sl_b,
                    'tp1': tp1_b
                    # Add other TPs if you implement partial exits in backtest
                }
        
        # Close any open trade at the end of backtest period
        if active_trade_details:
            exit_price = df_hist_full['close'].iloc[-1]
            pnl_pct = 0
            if active_trade_details['direction'] == 1:
                pnl_pct = (exit_price - active_trade_details['entry']) / active_trade_details['entry']
            else:
                pnl_pct = (active_trade_details['entry'] - exit_price) / active_trade_details['entry']
            balance *= (1 + pnl_pct)
            trades.append({
                'entry_time': active_trade_details['entry_time'], 'entry_price': active_trade_details['entry'],
                'exit_time': df_hist_full.index[-1], 'exit_price': exit_price,
                'profit_pct': pnl_pct * 100, 'status': 'ClosedAtEnd'
            })

        total_trades = len(trades)
        if total_trades > 0:
            winning_trades = sum(1 for t in trades if t['profit_pct'] > 0)
            win_rate = (winning_trades / total_trades) * 100
            avg_profit_pct = np.mean([t['profit_pct'] for t in trades])
            total_return_pct = (balance - initial_balance) / initial_balance * 100
            report = {
                'symbol': symbol, 'period': f"{days} days (SIMPLIFIED BACKTEST)", 
                'total_trades': total_trades, 'win_rate': win_rate, 
                'avg_profit_pct': avg_profit_pct, 'total_return_pct': total_return_pct,
                'final_balance': balance
            }
            return report, trades
        else:
            return {'symbol': symbol, 'period': f"{days} days (SIMPLIFIED BACKTEST)", 'total_trades': 0, 'message': 'No trades in simplified backtest'}, []


    def _timeframe_to_minutes(self, timeframe_str): # Your existing method is fine
        if not timeframe_str or not isinstance(timeframe_str, str) or len(timeframe_str) < 2:
            logger.warning(f"Invalid timeframe string: {timeframe_str}. Defaulting to 60 min.")
            return 60 
        unit = timeframe_str[-1].lower()
        try:
            value = int(timeframe_str[:-1])
        except ValueError:
            logger.warning(f"Invalid timeframe numeric value: {timeframe_str}. Defaulting to 60 min.")
            return 60

        if unit == 'm': return value
        elif unit == 'h': return value * 60
        elif unit == 'd': return value * 24 * 60
        else: 
            logger.warning(f"Unknown timeframe unit: {unit} in {timeframe_str}. Defaulting to 60 min.")
            return 60

    def run_bot(self, interval=None, backtest_days=None):
        # Interval from config or default
        if interval is None:
            interval = self.config.getint('TRADING', 'RUN_INTERVAL_SECONDS', fallback=300) # e.g. 5 minutes

        if backtest_days is not None and backtest_days > 0 :
            logger.info(f"Running backtest for all symbols for {backtest_days} days...")
            for symbol in self.symbols:
                report, trades_summary = self.backtest_strategy(symbol, days=backtest_days)
                # ... (rest of your backtest reporting logic - ensure it handles new report format) ...
                # This part needs to be robust to use the `format_value` properly
                report_message = f"ðŸ§ª *Backtest: {symbol} ({report.get('period', 'N/A')})* ðŸ§ª\n"
                for key, val in report.items():
                    if key not in ['symbol', 'period', 'message']: # Avoid duplicating these
                         report_message += f"*{key.replace('_', ' ').title()}:* {str(val)}\n" # Simple formatting
                if 'message' in report: report_message += f"Message: {report['message']}\n"
                self.send_telegram_info(report_message)
                time.sleep(2) # Avoid hitting Telegram rate limits
            logger.info("All backtests complete. Exiting as backtest_days was specified.")
            return # Exit after backtesting
        
        logger.info(f"Starting real-time monitoring. Interval: {interval}s")
        self.send_telegram_info(f"â–¶ï¸ *Real-time Enhanced Monitoring Started*\nInterval: {interval}s")
        
        # Min candles for primary TF before attempting signal generation
        min_candles_primary_tf = self.config.getint('TRADING', 'MIN_CANDLES_FOR_SIGNAL', fallback=50)
        resend_signal_threshold_seconds = self.config.getint('STRATEGY', 'RESEND_SIGNAL_THRESHOLD_SECONDS', fallback=1800) # 30 mins

        try:
            while True:
                current_loop_time = datetime.now()
                logger.info(f"--- New Monitoring Cycle @ {current_loop_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
                try:
                    for symbol in self.symbols:
                        logger.debug(f"Processing symbol: {symbol}")
                        # --- 1. Manage Active Trades (SL, TP, Trailing SL, Partial TPs) ---
                        df_primary_live = self.fetch_ohlcv_data(symbol, timeframe=self.default_ohlcv_timeframe, limit=self.atr_period_risk + 5) # Need enough for ATR
                        if df_primary_live.empty:
                            logger.warning(f"Could not fetch live data for {symbol} to manage active trades.")
                            time.sleep(1) # Small pause before next symbol
                            continue
                        
                        df_primary_live_indic = self.calculate_indicators(df_primary_live)
                        if df_primary_live_indic.empty:
                            logger.warning(f"Could not calculate live indicators for {symbol} to manage active trades.")
                            time.sleep(1)
                            continue
                        
                        current_price = df_primary_live_indic['close'].iloc[-1]
                        current_atr = df_primary_live_indic['atr'].iloc[-1] if 'atr' in df_primary_live_indic.columns and not pd.isna(df_primary_live_indic['atr'].iloc[-1]) else 0

                        # Manage active trades for this symbol
                        for trade_idx, trade in enumerate(list(self.active_trades[symbol])): # Iterate copy for safe removal
                            trade_hit_status = None # SL, TP1, TP2, TP3
                            profit_pct = 0.0
                            exit_price_for_calc = current_price # Assume current price for SL/Trailing SL exit

                            # A. Check Stop Loss (Original or Trailing)
                            current_stop_loss = trade['trailing_sl'] if self.enable_trailing_stop and 'trailing_sl' in trade else trade['sl']
                            
                            if (trade['direction'] == 1 and current_price <= current_stop_loss) or \
                               (trade['direction'] == -1 and current_price >= current_stop_loss):
                                trade_hit_status = "SL"
                                exit_price_for_calc = current_stop_loss # Exit at SL price
                            
                            # B. Check Take Profits (Partial or Full)
                            # Assumes 'tp_levels_hit' is a list like [False, False, False] for TP1, TP2, TP3
                            if not trade_hit_status: # Only check TPs if SL not hit
                                tps_to_check = [('TP1', trade['tp1']), ('TP2', trade['tp2']), ('TP3', trade['tp3'])]
                                partial_tp_percentages_str = self.config.get('RISK', 'PARTIAL_TP_PERCENTAGES', fallback='0.33,0.33,0.34').split(',')
                                partial_tp_percentages = [float(p.strip()) for p in partial_tp_percentages_str]
                                if len(partial_tp_percentages) != 3 : partial_tp_percentages = [0.33,0.33,0.34] # Fallback

                                for i, (tp_level_name, tp_price) in enumerate(tps_to_check):
                                    if not trade['tp_levels_hit'][i]: # If this TP level not yet hit
                                        if (trade['direction'] == 1 and current_price >= tp_price) or \
                                           (trade['direction'] == -1 and current_price <= tp_price):
                                            
                                            trade_hit_status = tp_level_name # e.g., "TP1"
                                            exit_price_for_calc = tp_price # Exit at TP price
                                            
                                            portion_to_close = partial_tp_percentages[i]
                                            # If it's the last TP level or only one TP defined, close full remaining position
                                            if i == len(tps_to_check) - 1 or sum(trade['tp_levels_hit']) == len(tps_to_check) -1:
                                                portion_to_close = 1.0 # Close remaining

                                            actual_amount_closed = trade['remaining_size'] * portion_to_close
                                            trade['remaining_size'] -= actual_amount_closed
                                            trade['tp_levels_hit'][i] = True
                                            
                                            # Optional: Move SL to BreakEven or previous TP after TP1
                                            if i == 0: # After TP1
                                                if trade['direction'] == 1: trade['sl'] = max(trade['sl'], trade['entry_price']) # Move SL to BE
                                                else: trade['sl'] = min(trade['sl'], trade['entry_price'])
                                                if self.enable_trailing_stop and 'trailing_sl' in trade: # Also update trailing SL base
                                                     trade['trailing_sl_base_price'] = trade['entry_price']

                                            logger.info(f"{symbol} - {tp_level_name} hit. Closed {portion_to_close*100:.0f}% of remaining. New size: {trade['remaining_size']:.4f}")
                                            break # Process one TP hit per cycle for this trade
                            
                            # C. Update Trailing Stop Loss (if enabled and no TP/SL hit yet)
                            if not trade_hit_status and self.enable_trailing_stop and current_atr > 0:
                                new_trailing_sl = trade.get('trailing_sl', trade['sl']) # Start with original SL if no trailing yet
                                trailing_sl_base = trade.get('trailing_sl_base_price', trade['entry_price'])

                                if trade['direction'] == 1: # Long
                                    # Move trailing SL up if price moves up significantly from base
                                    potential_tsl = current_price - (current_atr * self.trailing_stop_atr_multiplier)
                                    if potential_tsl > new_trailing_sl and potential_tsl > trailing_sl_base : # Ensure it only moves up and past base
                                        trade['trailing_sl'] = potential_tsl
                                        trade['trailing_sl_base_price'] = current_price # Update base price for next TSL calc
                                        logger.debug(f"{symbol} Long - Trailing SL updated to {trade['trailing_sl']:.4f}")
                                elif trade['direction'] == -1: # Short
                                    potential_tsl = current_price + (current_atr * self.trailing_stop_atr_multiplier)
                                    if potential_tsl < new_trailing_sl and potential_tsl < trailing_sl_base:
                                        trade['trailing_sl'] = potential_tsl
                                        trade['trailing_sl_base_price'] = current_price
                                        logger.debug(f"{symbol} Short - Trailing SL updated to {trade['trailing_sl']:.4f}")


                            # D. Process Trade Closure (if SL or any TP hit)
                            if trade_hit_status:
                                if trade['direction'] == 1: # Long
                                    profit_pct = (exit_price_for_calc - trade['entry_price']) / trade['entry_price'] * 100
                                else: # Short
                                    profit_pct = (trade['entry_price'] - exit_price_for_calc) / trade['entry_price'] * 100
                                
                                original_signal_link = self.get_telegram_message_link(trade.get('signal_chat_id'), trade.get('signal_msg_id'))
                                link_text = f"\n[ðŸ”— Original Signal]({original_signal_link})" if original_signal_link else ""
                                
                                status_emoji = "â›”" if trade_hit_status == "SL" else "ðŸ’°"
                                closure_message = (f"{status_emoji} *TRADE {trade_hit_status.upper()}* {status_emoji}\n"
                                                   f"*{symbol}* at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                                                   f"*Direction:* {'Long' if trade['direction'] == 1 else 'Short'}\n"
                                                   f"*Entry:* ${trade['entry_price']:.4f}\n"
                                                   f"*Exit Price:* ${exit_price_for_calc:.4f} (Target: ${trade[trade_hit_status.lower()] if trade_hit_status != 'SL' else current_stop_loss:.4f})\n"
                                                   f"*P/L on this part:* {profit_pct:.2f}%{link_text}")
                                self.send_telegram_info(closure_message)

                                if trade_hit_status != "SL": self.successful_signals_count[symbol] +=1 # Count TPs as success

                                # Record this part of the trade
                                trade_part_result = {
                                    'entry_time': trade['entry_time'].strftime("%Y-%m-%d %H:%M:%S"), 
                                    'entry_price': float(trade['entry_price']),
                                    'exit_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 
                                    'exit_price': float(exit_price_for_calc),
                                    'profit_pct': float(profit_pct), 
                                    'exit_reason': trade_hit_status,
                                    'direction': 'Long' if trade['direction'] == 1 else 'Short',
                                    'size_closed_pct': portion_to_close * 100 if "TP" in trade_hit_status else 100.0
                                }
                                self.price_history[symbol]['trade_results'].append(trade_part_result)
                                
                                # If all portions closed (remaining_size is near zero) or SL hit, remove trade
                                if trade['remaining_size'] < 0.0001 or trade_hit_status == "SL": # Using small threshold for float comparison
                                    self.active_trades[symbol].pop(trade_idx) # Remove by index from original list
                                    logger.info(f"Fully closed trade for {symbol} due to {trade_hit_status}.")
                                else:
                                    logger.info(f"{symbol} - Partial TP hit. Remaining size: {trade['remaining_size']:.4f}")
                                
                                self.save_price_history(symbol)
                                continue # Move to next active trade or next symbol

                            # E. Time-based alert (already in your script, good)
                            expected_duration_minutes = trade.get('expected_trade_duration_minutes', 240) # e.g., 4 hours default
                            if not trade.get('time_alert_sent') and \
                               (datetime.now() - trade['entry_time']).total_seconds() / 60 > expected_duration_minutes * 1.5: # Alert if 1.5x expected time
                                current_pnl_active = ((current_price - trade['entry_price']) / trade['entry_price'] * 100 if trade['direction'] == 1 else (trade['entry_price'] - current_price) / trade['entry_price'] * 100)
                                reminder_msg = f"â° *TRADE DURATION ALERT* â°\n*{symbol}* trade active longer than expected.\n*Entry:* ${trade['entry_price']:.4f}\n*Current P/L:* {current_pnl_active:.2f}%"
                                self.send_telegram_info(reminder_msg)
                                self.active_trades[symbol][trade_idx]['time_alert_sent'] = True # Mark alert sent for this specific trade object


                        # --- 2. Generate New Signals (if no active trade for this symbol or to add to existing if logic allows) ---
                        # For now, assume one trade per symbol at a time to keep it simpler
                        if not self.active_trades[symbol]: 
                            df_main_tf_for_signal = self.fetch_ohlcv_data(symbol, timeframe=self.default_ohlcv_timeframe, limit=self.config.getint('TRADING','MIN_CANDLES_FOR_SIGNAL',fallback=100) + 50) # Fetch more for indicators
                            if df_main_tf_for_signal.empty or len(df_main_tf_for_signal) < min_candles_primary_tf:
                                logger.warning(f"Not enough data for {symbol} to generate new signal ({len(df_main_tf_for_signal)} candles).")
                                time.sleep(1)
                                continue
                            
                            # Calculate indicators on this potentially larger dataset
                            df_main_tf_for_signal_indic = self.calculate_indicators(df_main_tf_for_signal)
                            if df_main_tf_for_signal_indic.empty:
                                logger.warning(f"Indicator calculation failed for {symbol} for new signal generation.")
                                time.sleep(1)
                                continue

                            self.update_price_history(df_main_tf_for_signal_indic, symbol) # Store features from this data
                            
                            trade_signal_details = self.generate_final_trade_signal(symbol) # This now uses data_all_tf internally

                            if trade_signal_details:
                                signal_dir_numeric = 1 if trade_signal_details['direction'] == 'Long' else -1
                                time_since_last = float('inf')
                                if self.last_signal_time[symbol]:
                                    time_since_last = (datetime.now() - self.last_signal_time[symbol]).total_seconds()

                                if self.last_signal_direction[symbol] != signal_dir_numeric or time_since_last > resend_signal_threshold_seconds:
                                    market_context_live = self.get_market_context(df_main_tf_for_signal_indic, symbol) # Get context from primary TF
                                    
                                    # Format pattern details for message
                                    patterns_msg_part = ""
                                    if trade_signal_details.get('pattern_details'):
                                        patterns_msg_part = "\nðŸ” *Detected Patterns (Primary TF):*\n"
                                        for p_idx, p_info in enumerate(trade_signal_details['pattern_details'][:2]): # Show top 2
                                            patterns_msg_part += f"  - {p_info.get('name','Unknown')} (Sig: {p_info.get('signal',0)}, Conf: {p_info.get('confidence',0):.2f})\n"


                                    signal_message_text = (
                                        f" ÑÐ¸Ð³Ð½Ð°Ð»Ñ‹ {'Long' if signal_dir_numeric == 1 else 'Short'} {symbol} ðŸš€\n\n"
                                        f"Entry: ${trade_signal_details['entry_price']:.4f}\n"
                                        f"Size: {trade_signal_details['position_size']:.4f} units\n\n"
                                        f"ðŸŽ¯ TP1: ${trade_signal_details['TP1']:.4f}\n"
                                        f"ðŸŽ¯ TP2: ${trade_signal_details['TP2']:.4f}\n"
                                        f"ðŸŽ¯ TP3: ${trade_signal_details['TP3']:.4f}\n"
                                        f"ðŸ›‘ SL: ${trade_signal_details['SL']:.4f}\n\n"
                                        f"ðŸ“Š Context ({self.default_ohlcv_timeframe}):\n"
                                        f"  Price: ${market_context_live['price']:.4f}, 24h Chg: {market_context_live['price_change_24h']:.2f}%\n"
                                        f"  ATR%: {market_context_live['volatility_atr_pct']:.2f}%, RSI: {market_context_live['rsi']:.2f}\n"
                                        f"  Trend: {market_context_live['trend_ema']}, BB: {market_context_live['bollinger_position']}\n"
                                        f"  ADX: {market_context_live['adx']:.2f}, Regime: {trade_signal_details['market_regime']}\n"
                                        f"  Sentiment: {trade_signal_details['sentiment_score']:.3f}\n"
                                        f"  ML Score: {trade_signal_details['ml_reliability_score']:.3f} (Rule Str: {trade_signal_details['rule_based_signal_strength']:.2f})\n"
                                        f"{patterns_msg_part}\n"
                                        f"Confidence: {'ðŸ”¥'*int(max(1,min(5,trade_signal_details.get('final_confidence_score',1)*2.5)))}" # Visual confidence
                                    )
                                    
                                    sent_telegram_obj = self.send_signal_message(signal_message_text)
                                    msg_id, chat_id_tel = (sent_telegram_obj.message_id, sent_telegram_obj.chat.id) if sent_telegram_obj else (None, None)

                                    # Prepare trade object for active_trades
                                    new_trade_entry = {
                                        'symbol': symbol,
                                        'entry_time': datetime.now(),
                                        'entry_price': float(trade_signal_details['entry_price']),
                                        'direction': signal_dir_numeric,
                                        'sl': float(trade_signal_details['SL']),
                                        'tp1': float(trade_signal_details['TP1']),
                                        'tp2': float(trade_signal_details['TP2']),
                                        'tp3': float(trade_signal_details['TP3']),
                                        'initial_size': float(trade_signal_details['position_size']),
                                        'remaining_size': float(trade_signal_details['position_size']),
                                        'tp_levels_hit': [False, False, False], # For partial TPs
                                        'trailing_sl': float(trade_signal_details['SL']), # Initial trailing SL is the SL
                                        'trailing_sl_base_price': float(trade_signal_details['entry_price']), # Initial base for TSL calc
                                        'expected_trade_duration_minutes': self._timeframe_to_minutes(self.default_ohlcv_timeframe) * 4, # e.g. 4 candles of primary TF
                                        'signal_msg_id': msg_id, 
                                        'signal_chat_id': chat_id_tel,
                                        'time_alert_sent': False,
                                        'market_regime_at_entry': trade_signal_details['market_regime'],
                                        'sentiment_at_entry': trade_signal_details['sentiment_score'],
                                        'ml_score_at_entry': trade_signal_details['ml_reliability_score']
                                    }
                                    self.active_trades[symbol].append(new_trade_entry)
                                    self.last_signal_time[symbol] = datetime.now()
                                    self.last_signal_direction[symbol] = signal_dir_numeric
                                    self.signals_generated_count[symbol] += 1
                                    logger.info(f"{symbol} - NEW SIGNAL: {trade_signal_details['direction']} @ {new_trade_entry['entry_price']:.4f}, Size: {new_trade_entry['initial_size']:.4f}")
                                else:
                                    logger.info(f"{symbol} - Signal ({trade_signal_details['direction']}) same as last or too recent. Not resending/re-entering.")
                        time.sleep(1) # Small pause between symbols within a loop

                except ccxt.NetworkError as e:
                    logger.error(f"Network error in monitoring loop: {str(e)}. Retrying after {interval}s.")
                    self.send_telegram_info(f"âš ï¸ *Warning:* Network error: {str(e)}. Bot continues...")
                    time.sleep(interval / 2) # Shorter sleep on recoverable errors
                except ccxt.ExchangeError as e:
                    logger.error(f"Exchange error in monitoring loop: {str(e)}. Retrying after {interval}s.")
                    self.send_telegram_info(f"âš ï¸ *Warning:* Exchange error: {str(e)}. Bot continues...")
                    time.sleep(interval / 2)
                except Exception as e:
                    logger.error(f"Critical error in monitoring loop for symbol {symbol if 'symbol' in locals() else 'N/A'}: {str(e)}", exc_info=True)
                    self.send_telegram_info(f"âš ï¸ *ERROR:* General error in monitoring loop: {str(e)}. Bot continues but check logs!")
                    time.sleep(interval) 
                
                # Save price history periodically for all symbols
                if current_loop_time.minute % 15 == 0: # Every 15 minutes
                    logger.info("Periodic save of all price histories...")
                    for sym_to_save in self.symbols:
                        self.save_price_history(sym_to_save)
                
                logger.info(f"--- Cycle ended. Waiting {interval}s ---")
                time.sleep(interval)
            
        except KeyboardInterrupt:
            logger.info("Bot stopping due to KeyboardInterrupt...")
            self.send_telegram_info("ðŸ›‘ *Bot Stopped (Keyboard Interrupt)*")
        finally:
            logger.info("Saving all price histories before full exit...")
            for symbol_to_save in self.symbols:
                if self.price_history.get(symbol_to_save): 
                    self.save_price_history(symbol_to_save)
            logger.info("All price histories saved. Bot shutdown complete.")
            self.send_telegram_info("ðŸ”Œ Bot Shutdown Complete.")

# --- Helper: ML Model Training and Regime Clustering (Example - Run Offline) ---
def train_ml_reliability_model(bot_instance, symbol_to_train, output_dir="ml_models"):
    """
    Conceptual function to train the ML reliability model.
    This needs to be run offline with significant historical data.
    """
    logger.info(f"--- Starting ML Reliability Model Training for {symbol_to_train} (Conceptual) ---")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. Load historical features and trade results
    # The `historical_features` in price_history is a simple example.
    # You need to generate comprehensive features for each candle in your history
    # that corresponds to a potential trade setup, and then label it based on outcome.
    
    # This is a placeholder for actual data loading and feature engineering.
    # You would iterate through historical OHLCV, detect patterns, calculate indicators,
    # and then see if a trade taken at that point would have been successful (hit TP before SL).
    
    # Example features: df_features (DataFrame with columns like 'rsi', 'macd_hist', 'pattern_type_onehot_...')
    # Example target: df_target (Series with 1 for success, 0 for failure)
    
    # For demonstration, let's assume you have a CSV like 'symbol_ml_training_data.csv'
    # with columns: rsi,macd_hist,...,pattern_signal,...,sentiment_score, trade_outcome (0 or 1)
    
    # This part is highly conceptual and requires you to build the data generation pipeline.
    # For now, we'll skip to model fitting assuming `X_train, y_train` exist.
    
    # Placeholder: Generating dummy data for demonstration
    num_samples = 1000
    num_features = 16 # Must match _prepare_ml_features output + direction
    X_train_dummy = np.random.rand(num_samples, num_features)
    y_train_dummy = np.random.randint(0, 2, num_samples) # 0 for fail, 1 for success
    
    if len(y_train_dummy) < 100 or len(np.unique(y_train_dummy)) < 2:
        logger.error("Not enough diverse data to train ML reliability model. Skipping.")
        return

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_dummy)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    model.fit(X_train_scaled, y_train_dummy)
    
    model_path = os.path.join(output_dir, bot_instance.config.get('ML_MODELS', 'RELIABILITY_MODEL_PATH', fallback='ml_reliability_model.joblib'))
    scaler_path = os.path.join(output_dir, bot_instance.config.get('ML_MODELS', 'RELIABILITY_SCALER_PATH', fallback='ml_reliability_scaler.joblib'))
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    logger.info(f"ML Reliability Model and Scaler saved to {model_path}, {scaler_path}")
    logger.info("IMPORTANT: This is a DUMMY model. Train with real, comprehensive historical data.")


def train_market_regime_model(bot_instance, symbol_to_train, n_clusters=3, output_dir="ml_models"):
    """Conceptual function to train K-Means for market regime."""
    logger.info(f"--- Starting Market Regime Model Training for {symbol_to_train} (Conceptual) ---")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Fetch a good amount of historical data for feature extraction
    # df_hist = bot_instance.fetch_ohlcv_data(symbol_to_train, bot_instance.default_ohlcv_timeframe, limit=2000)
    # df_indic = bot_instance.calculate_indicators(df_hist)
    # if df_indic.empty or len(df_indic) < 100:
    #     logger.error("Not enough data for regime model training.")
    #     return

    # features_list = []
    # for i in range(len(df_indic)):
    #     regime_features = bot_instance._get_regime_features(df_indic.iloc[[i]]) # Pass as DataFrame
    #     if regime_features is not None:
    #         features_list.append(regime_features.flatten())
    # if not features_list: 
    #     logger.error("No regime features extracted."); return
    
    # X_regime = np.array(features_list)

    # Placeholder: Dummy data for regime features (e.g., ATR%, ADX)
    num_samples_regime = 1000
    num_regime_features = 2 # e.g., ATR_pct, ADX from _get_regime_features
    X_regime_dummy = np.random.rand(num_samples_regime, num_regime_features) * np.array([5, 70]) # Scale to typical ATR%, ADX ranges


    if len(X_regime_dummy) < n_clusters * 10:
        logger.error("Not enough data for K-Means regime clustering.")
        return

    scaler = StandardScaler()
    X_regime_scaled = scaler.fit_transform(X_regime_dummy)
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(X_regime_scaled)
    
    model_path = os.path.join(output_dir, bot_instance.config.get('ML_MODELS', 'REGIME_MODEL_PATH', fallback='market_regime_model.joblib'))
    scaler_path = os.path.join(output_dir, bot_instance.config.get('ML_MODELS', 'REGIME_SCALER_PATH', fallback='market_regime_scaler.joblib'))
    joblib.dump(kmeans, model_path)
    joblib.dump(scaler, scaler_path)
    logger.info(f"Market Regime K-Means Model and Scaler saved to {model_path}, {scaler_path}")
    logger.info(f"Cluster centers (scaled):\n{kmeans.cluster_centers_}")
    logger.info("Interpret these clusters to define meaningful regime names (e.g., 'trending_bullish', 'ranging_low_vol').")
    logger.info("IMPORTANT: This is a DUMMY model. Train with real features from your historical data.")


if __name__ == "__main__":
    # Create a dummy config.txt if it doesn't exist for the first run
    if not os.path.exists("config.txt"):
        bot_temp_for_config = KuCoinSwingBotEnhanced() # This will call _create_default_config

    bot = KuCoinSwingBotEnhanced(config_file="config.txt")
    
    # --- OFFLINE TRAINING STEPS (Run once or periodically) ---
    # You need to uncomment and run these after configuring your bot
    # and ensuring you have a way to generate/collect sufficient historical feature data.
    # For now, dummy models will be attempted to be loaded or fail gracefully.
    
    # Example: Train models for a primary symbol if they don't exist
    # primary_symbol_for_training = bot.symbols[0] if bot.symbols else "BTC/USDT"
    # if not os.path.exists(bot.config.get('ML_MODELS','RELIABILITY_MODEL_PATH', fallback='dummy')) : # Check if model exists
    #    logger.info(f"Attempting to train dummy ML reliability model for {primary_symbol_for_training}...")
    #    train_ml_reliability_model(bot, primary_symbol_for_training, output_dir="ml_models")
    # if not os.path.exists(bot.config.get('ML_MODELS','REGIME_MODEL_PATH', fallback='dummy')):
    #    logger.info(f"Attempting to train dummy market regime model for {primary_symbol_for_training}...")
    #    train_market_regime_model(bot, primary_symbol_for_training, n_clusters=3, output_dir="ml_models") # Adjust n_clusters
    # --- END OFFLINE TRAINING STEPS ---

    # Load models again after potential training
    bot._load_ml_models()


    run_backtest_days = bot.config.getint('TRADING', 'RUN_BACKTEST_DAYS_ON_START', fallback=0)
    if run_backtest_days > 0:
        bot.run_bot(backtest_days=run_backtest_days) # Bot will exit after backtest
    else:
        run_interval_seconds = bot.config.getint('TRADING', 'RUN_INTERVAL_SECONDS', fallback=300)
        bot.run_bot(interval=run_interval_seconds)
